{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "### Things that need to be done    \n",
    "\n",
    "- [x] Set up skeleton for a nice object oriented approach. \n",
    "- [x] Figure out best way to get tweets.\n",
    "- [x] Data format\n",
    "- [x] Build class and funcs\n",
    "- [x] Figure out how to best count all requests in a session and make sure it's functional\n",
    "    - This is already built-in to searchtweets to some extent, so will use that.\n",
    "- [x] Make oneDay() fail safe (it's a bad idea to get the data and combine it within a single for, because if something goes wrong in an interation all the data from previous iterations will be lost if something goes wrong).\n",
    "    - Getting a whole week instead\n",
    "- [x] Change oneDay() to oneWeek().    \n",
    "- [x] Save metadata for the payloads (i.e. self.most_recent, self.oldest, self.timeCovered, rs.total_results + other?)\n",
    "    - Seach tweets already saves some sort of log, print this to file with the appropriate name.  \n",
    "- [ ] More tests on oneWeek(): currently it sometimes returns an empty Stream sometimes, it might be due to bad handling of rate limits by searchtweets?\n",
    "- [ ] Updated export and loading to be compliant with the new folder structure. \n",
    "- [ ] See why df3 different size than what we get with oneWeek(). Test in intervals > 15 min, my suspicion is that it's bad handling of rate limits by searchtweets.\n",
    "\n",
    "### Rate Limits\n",
    "\n",
    "- 10,000,000 Tweets per month (resets on the 19th of each month). \n",
    "- 300 requests/15 minute window, with 500 Tweets/request:\n",
    "    - 150,000 tweets/15min \n",
    "    - 600,000 tweets/hour\n",
    "\n",
    "### How many tweets to get?\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - ~ 280 days\n",
    "    - ~ 6720 hours\n",
    "    - If we get 1000 tweets per hour: $6,720,000 * 2$. \n",
    "    - That's very little in terms of space, but might take quite a while for it to go through sentiment analysis.\n",
    "    - It would take ~22 hours to get the whole data (due to rate limits).\n",
    "    - However, it's unlikely that our queries would return anywhere near 1,000 results/hour.\n",
    "\n",
    "### Best way to get tweets\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - $n_h$ per hour/day\n",
    "    - $n_d$ per day (where $n_d$ would be ~ $n_h*24$)\n",
    "    - $n_w$ per week (where $n_w$ would be ~ $n_h*24 *7$)  \n",
    "    \n",
    "\n",
    "- $n_w$ is probably the best options: \n",
    "    - can leverage functions built into *searchtweets* to avoid rate limit violations (e.g. exponential back-off).\n",
    "    - it's easy to select tweets in any given day/hour from these data.\n",
    "\n",
    "### Data format\n",
    "\n",
    "- A single results call: **JSON to pd**.\n",
    "    - This is relatively straightforward with one minor complication, i.e. entries such as this:\n",
    "    <blockquote>{'newest_id': '1402310241992183808',\n",
    "  'oldest_id': '1402310139630211083',\n",
    "  'result_count': 100,\n",
    "  'next_token': 'b26v89c19zqg8o3fpdg7rbcqdq8stpgmibslekg3kxail'}\n",
    "    </blockquote>\n",
    "    - This is used by the wrapper to get the next lot of tweets if max_tweets > results_per_call, but will also always be the last entry in a result.\n",
    "    \n",
    "    \n",
    "- Multiple result calls: **pds in dict/dict-of-dict**. \n",
    "    - I am thinking the best way to store all the data would be a dict of dataframes, but will see how it works  \n",
    "    \n",
    "### Survey periods\n",
    "\n",
    "| Period | A_I_start_date | A_I_end_date | A_I_week | HPS_start_date | HPS_end_date | HPS_Week | HPS Topic |\n",
    "|--------|----------------|--------------|----------|----------------|--------------|----------|-----------|\n",
    "| P1     | 23.10.2020     | 26.10.2020   | W29*     | 28.10.2020     | 09.11.2020   | W18      |     E    |\n",
    "| P2     | 13.11.2020     | 16.11.2020   | W30      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P2     | 20.11.2020     | 23.11.2020   | W31      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P3     | 04.12.2020     | 07.12.2020   | W32      | 25.11.2020     | 07.12.2020   | W20      |     E    |\n",
    "| P4     | 11.12.2020     | 14.12.2020   | W33      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P4     | 18.12.2020     | 21.12.2020   | W34      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P5     | 08.01.2021     | 11.01.2021   | W35      | 06.01.2021     | 18.01.2021   | W22      |    E,V   |\n",
    "| P6     | 22.01.2021     | 25.01.2021   | W36      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P6     | 29.01.2021     | 01.02.2021   | W37      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P7     | 05.02.2021     | 08.02.2021   | W38      | 03.02.2021     | 15.02.2021   | W24      |    E,V   |\n",
    "| P8     | 19.02.2021     | 22.02.2021   | W39      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P8     | 28.02.2021     | 01.03.2021   | W40      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P9     | 05.03.2021     | 08.03.2021   | W41      | 03.03.2021     | 15.03.2021   | W26      |    E,V   |\n",
    "| P10    | 19.03.2021     | 22.03.2021   | W42      | 17.03.2021     | 29.03.2021   | W27      |    E,V   |\n",
    "| P11    | 02.04.2021     | 05.04.2021   | W43      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P11    | 16.04.2021     | 19.04.2021   | W44      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P12    | 07.05.2021     | 10.05.2021   | W45      | 28.04.2021     | 10.05.2021   | W29      |    E,V   |\n",
    "| P13    | 21.05.2021     | 24.05.2021   | W46      |                |              | W30      |    E,V   |\n",
    "\n",
    "\n",
    "e.g. P1: 23.10.20 - 26.10.20 (Fri - Mon)\n",
    "* Corresponding week 19.19.20 - 25.10.20 or 26.10.20 - 01.11.20?\n",
    "* For now let's say the former. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "from os import path\n",
    "from searchtweets import ResultStream, gen_request_parameters, load_credentials, collect_results, convert_utc_time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2021-05-24 we have 213 days, 5112 hours, and 51120 tweets (with 10 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "def countTweets(startDate, endDate, tweets_per_hour):\n",
    "    '''\n",
    "    Specify dates in DD.MM.YYY format (no leading 0 for months or days)\n",
    "    '''\n",
    "    \n",
    "    s_d, s_m, s_y = [ int(i) for i in startDate.split('.')]\n",
    "    e_d, e_m, e_y = [ int(i) for i in endDate.split('.')]\n",
    "\n",
    "    endDate = date(e_y, e_m, e_d)\n",
    "    startDate = date(s_y, s_m, s_d)\n",
    "    days = endDate-startDate\n",
    "    print(\"From {} to {} we have {} days, {} hours, and {} tweets (with {} tweets per hour)\".format(startDate, \n",
    "                                                                                                    endDate, \n",
    "                                                                                                    days.days, \n",
    "                                                                                                    days.days*24, \n",
    "                                                                                                   days.days*24*tweets_per_hour,\n",
    "                                                                                                   tweets_per_hour))\n",
    "    \n",
    "    \n",
    "countTweets('23.10.2020', '24.05.2021', 10)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2020-11-01 we have 9 days, 216 hours, and 216000 tweets (with 1000 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "countTweets('23.10.2020', '1.11.2020', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitterData():\n",
    "    '''\n",
    "    A class for holding all the Twitter search related elements, from validating credentials\n",
    "    to cleaning the data.\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, main_path = '/Volumes/Survey_Social_Media_Compare/Methods/'):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.main_path = main_path\n",
    "    \n",
    "    def validate_credentials(self):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        c_path = path.join(self.main_path, 'Scripts/Twitter/twitter_keys.yaml')\n",
    "        self.credentials = load_credentials(c_path, \n",
    "                                       env_overwrite=True);\n",
    "        self.all_requests = 0;\n",
    "        self.total_results_overall = 0;\n",
    "\n",
    "        return \"Credentials validated successfully\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_query(self,\n",
    "                    mainTerms, \n",
    "                    startDate,\n",
    "                    endDate,\n",
    "                    inQuotes = True, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []):\n",
    "        \n",
    "        '''\n",
    "        Builds the query that is used to make the requests and get payloads.\n",
    "        \n",
    "        Parameters:\n",
    "            mainTerms (str): The search terms we want, e.g. 'jobs'\n",
    "            startDate (str): The lower end of the period we are interested in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 13:00'\n",
    "            endDate (str): The higher end of the period we are interested in in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 14:00'\n",
    "            inQuotes (bool): Do we want an exact phrase match? If true the terms will be put in quotes\n",
    "            language (str): Language used in the query (only languages supported by Twitter + \n",
    "                            has to be in the correct format, see https://bit.ly/2RBwmGa)\n",
    "            country (str): Country where Tweet/User is located (has to be in the correct format, see\n",
    "                            https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n",
    "            excludeRT (bool): Exclude retweets from the payload? Default False\n",
    "            results_per_call (int): How many results per request? Max is 500 for the academic API.\n",
    "            otherTerms (list): List of other search terms, e.g. ['#COVID', 'is:reply']\n",
    "        \n",
    "        Notes:\n",
    "            - More notes on building queries here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query.\n",
    "            - Tweets are fetched in reverse chronological order, i.e. starting at endDate \n",
    "            and continuing until a limit is reached.\n",
    "            - endDate refers to previous day until 23:59\n",
    "        '''\n",
    "        \n",
    "        # If excluding retweets, set rt to '-' \n",
    "        rt = '-is:retweet' if excludeRT == True else ''\n",
    "        \n",
    "        # Are the terms in quotes\n",
    "        mainTerms = '\"{}\"'.format(mainTerms) if inQuotes == True else '{}'\n",
    "        \n",
    "        # Build query text\n",
    "        queryText = '{} lang: {} place_country:{}'.format(mainTerms,\n",
    "                                                         language,\n",
    "                                                         country)\n",
    "        \n",
    "        # If there are other terms, include them in the queryText\n",
    "        queryText = queryText.extend(other) if otherTerms != [] else queryText\n",
    "        \n",
    "        # Save these as will be used to determine limits\n",
    "        self.results_per_call = results_per_call\n",
    "        \n",
    "        print(startDate)\n",
    "        print(endDate)\n",
    "            \n",
    "        # Build query\n",
    "        self.query = gen_request_parameters(queryText,\n",
    "                                      start_time = startDate,\n",
    "                                      end_time = endDate,\n",
    "                                      tweet_fields = return_fields,\n",
    "                                      results_per_call = self.results_per_call)\n",
    "        \n",
    "    \n",
    "    def get_data(self, nTweets):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.rs = ResultStream(request_parameters = self.query,\n",
    "                                  max_tweets = nTweets,\n",
    "                                  output_format = \"a\",\n",
    "                                  **self.credentials)\n",
    "        \n",
    "        self.result = list(self.rs.stream())\n",
    "        \n",
    "        # We can get the total requests made for a payload using:\n",
    "        # twitterData_instance.rs.n_requests\n",
    "        # twitterData_instance.rs.session_request_counter\n",
    "        \n",
    "        # This can be used to get the overall requests made and saving logs.\n",
    "        self.all_requests += self.rs.session_request_counter       \n",
    "        self.total_results_overall += self.rs.total_results\n",
    "        \n",
    "    \n",
    "    def surveyDates(self):\n",
    "        \n",
    "        # Path to survey periods file\n",
    "        s_path = path.join(self.main_path, 'Scripts/Surveys/table_details/surveyPeriods.xlsx')\n",
    "        \n",
    "        # Load survey periods\n",
    "        self.surveyPeriods = pd.read_excel(s_path, sheet_name='AI+HPS')\n",
    "        \n",
    "        # Generate tuple of week start and end dates based on the collection dates in the Axios/Ipsos survey. \n",
    "        self.AI_weeks = [twitterData.weekFromDay(date) for date in self.surveyPeriods['A_I_start_date']]\n",
    "        \n",
    "#         # Get first monday and last sunday from the A/I data collection periods\n",
    "#         firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "#         _, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "#         # Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "#         self.mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "#         self.sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "        \n",
    "#         # Get strings\n",
    "#         self.mondays_str = self.mondays.strftime('%Y-%m-%d')\n",
    "#         self.sundays_str = self.sundays.strftime('%Y-%m-%d')\n",
    "\n",
    "#         # Saving all the results in tuples\n",
    "#         self.all_weeks = [(m, s) for m, s in zip(self.mondays, self.sundays)]\n",
    "#         self.all_weeks_str = [(m, s) for m, s in zip(self.mondays_str, self.sundays_str)]\n",
    "\n",
    "        # Get first monday and last sunday from the A/I data collection periods\n",
    "        firstDate,_ = twitterData.weekFromDay(self.surveyPeriods['A_I_start_date'][0])\n",
    "        _, lastDate = twitterData.weekFromDay(self.surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "        # Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "        self.mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "#         self.sundays = pd.date_range(firstDate, lastDate, freq='W-SUN') # Legacy\n",
    "        self.leading_mondays = pd.date_range(twitterData.nextMonday(firstDate), twitterData.nextMonday(lastDate), freq='W-MON')\n",
    "\n",
    "        # Get strings\n",
    "        self.mondays_str = self.mondays.strftime('%Y-%m-%d')\n",
    "#         self.sundays_str = self.sundays.strftime('%Y-%m-%d') # Legacy\n",
    "        self.leading_mondays_str = self.leading_mondays.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Saving all the results in a tuple\n",
    "        self.all_weeks = [(m, s) for m, s in zip(self.mondays, self.leading_mondays)]\n",
    "        self.all_weeks_str = [(m, s) for m, s in zip(self.mondays_str, self.leading_mondays_str)]\n",
    "    \n",
    "    \n",
    "    def createDicts(self):\n",
    "        \n",
    "        self.allData = dict.fromkeys(self.mondays_str)\n",
    "        self.logs = dict.fromkeys(self.mondays_str)\n",
    "    \n",
    "    def oneWeek(self, mainTerms, weekNum):\n",
    "        '''\n",
    "        TODO: re-write this docstring to reflect changes. \n",
    "        Convenience function for getting all the tweets from a specified period.\n",
    "        The parameters are fed to **build_query()**, which has more parameters with the following default values:\n",
    "                    inQuotes = False, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []\n",
    "        These should either be added to the build_query() call within the current function, or the defaults changed in build_query().\n",
    "        Parameters:\n",
    "            mainTerms (str): search \n",
    "            startDate (str): week starting (format: 'YYYY-MM-DD' w, e.g. '2020-10-23')\n",
    "            endDate (str): week ending (~)\n",
    "            \n",
    "        Returns:\n",
    "            week_df (pd.DataFrame): Payload returned by the query for the specified period in df format.   \n",
    "        '''\n",
    "        \n",
    "        # Get start and end date from the week number\n",
    "        startDate = self.all_weeks_str[weekNum - 1][0]\n",
    "        endDate = self.all_weeks_str[weekNum - 1][1]\n",
    "\n",
    "        \n",
    "        # Could have also done\n",
    "        # startDate = self.mondays_str[weekNum - 1]\n",
    "        # endDate = self.leading_mondays[weekNum - 1]\n",
    "        \n",
    "        # Build the query with the specified terms\n",
    "        self.build_query(mainTerms, startDate, endDate, results_per_call=500)\n",
    "        \n",
    "        # Get the data. \n",
    "        self.get_data(nTweets = 20000) # 20,000 is quite conservative, it's unlikely we would get more than ~10,000/week.\n",
    "        \n",
    "        \n",
    "        # Clean data (-> pd.DataFrame) and save into dictionary, with the specified by the startDate (i.e. the date corresponding to Monday of any given week in the entire period covered)\n",
    "        self.allData[startDate] = twitterData.get_df(self.result)\n",
    "        \n",
    "        # Calculate the time covered in a payload.\n",
    "        # Most recent date/time in the df in datetime format\n",
    "        self.most_recent = twitterData.toDatetime(max(self.allData[startDate]['created_at']))\n",
    "        self.oldest = twitterData.toDatetime(min(self.allData[startDate]['created_at']))\n",
    "        \n",
    "        self.timeCovered = str(self.most_recent - self.oldest)\n",
    "        \n",
    "        self.logs[startDate] = {\n",
    "            'mostRecent': self.most_recent,\n",
    "            'oldest': self.oldest,\n",
    "            'timeCovered': self.timeCovered,\n",
    "            'sessionRequestCounter': self.rs.session_request_counter,\n",
    "            'totalRequests': self.all_requests,\n",
    "            'totalTweets': self.rs.total_results,\n",
    "            'totalTweetsOverall': self.total_results_overall,\n",
    "            'requestParams': self.rs.request_parameters\n",
    "            }\n",
    "            \n",
    "        # Save current week's Monday date.\n",
    "        # Will be used to name the files when saving.\n",
    "        self.currentWeek = startDate\n",
    "    \n",
    "    def exportOneWeek(self, topic, saveDF = True, saveJSON = False):\n",
    "        '''\n",
    "        topic (str): \"Employment\" or \"Vaccination\"\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if saveJSON:\n",
    "            json_path = path.join(self.main_path, 'Data/Twitter/{}/JSON/{}.json'.format(topic,self.currentWeek))\n",
    "            \n",
    "            with open(json_path, 'w') as fout:\n",
    "                json.dump(self.result, fout)\n",
    "            \n",
    "        if saveDF:\n",
    "            df_path = path.join(self.main_path, 'Data/Twitter/{}/CSV/{}.csv'.format(topic, self.currentWeek))\n",
    "            self.allData[self.currentWeek].to_csv(df_path)\n",
    "            \n",
    "    @staticmethod\n",
    "    def loadOneWeek(weekStart, topic, loadDF = True, loadJSON = False):\n",
    "        '''\n",
    "        weekStart (str): name of file to be loaded; same format as used for currentWeek, e.g. '2020-10-19'\n",
    "        topic (str): \"Employment\" or \"Vaccination\"\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if(loadDF and not loadJSON):\n",
    "            df = pd.read_csv('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/CSV/{}.csv'.format(topic, weekStart), index_col=0, dtype={'id': object})\n",
    "            return df\n",
    "        \n",
    "        if(not loadDF and loadJSON):\n",
    "            with open('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/JSON/{}.json'.format(topic, weekStart)) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        if(loadDF and loadJSON):\n",
    "            df = pd.read_csv('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/{}/CSV/{}.csv'.format(topic, weekStart), index_col=0, dtype={'id': object})\n",
    "            \n",
    "            with open('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/{}/JSON/{}.json'.format(topic, weekStart)) as f:\n",
    "                result = json.load(f)\n",
    "                    \n",
    "            return df, result                                                       \n",
    "    \n",
    "    @staticmethod\n",
    "    def toDatetime(dateStr):\n",
    "        '''\n",
    "        Take a date in the ISO format that we get from twitter \"%Y-%m-%dT%H:%M:%S.000Z\"\n",
    "        and transform to a datetime for calculations.\n",
    "\n",
    "        Parameters:\n",
    "            dateStr (str): A date string (ISO format)\n",
    "        \n",
    "        Returns:\n",
    "            dateDT (datetime): A datetime object  \n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            dateDT = datetime.strptime(dateStr, \"%Y-%m-%d\")\n",
    "            \n",
    "        except: \n",
    "            dateDT = datetime.strptime(dateStr, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "            \n",
    "        \n",
    "        return dateDT\n",
    "    \n",
    "    @staticmethod\n",
    "    def weekFromDay(day):\n",
    "        '''\n",
    "        Work the week starting and ending dates given any date.\n",
    "        Params:\n",
    "            day (datetime): Can be a Timestamp (pandas/numpy object) or a datetime.datetime object.\n",
    "\n",
    "        Returns: \n",
    "            weekStart (Timestamp): The date corresponding to the start (i.e. Monday) of the date specified by *day* param.\n",
    "            weekEnd (Timestamp): The date corresponding to the end (i.e. Sunday) of the date specified by *day* param.\n",
    "        '''\n",
    "\n",
    "        weekStart = day - timedelta(days=day.weekday())\n",
    "        weekEnd = weekStart + timedelta(days=6)\n",
    "\n",
    "        return weekStart.strftime('%Y-%m-%d'), weekEnd.strftime('%Y-%m-%d')\n",
    "    \n",
    "    @staticmethod\n",
    "    def nextMonday(date):\n",
    "        date = twitterData.toDatetime(date)\n",
    "\n",
    "        nextM = date + timedelta(days=-date.weekday(), weeks=1)\n",
    "\n",
    "        return nextM\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_df(dictLS):\n",
    "        '''\n",
    "        '''\n",
    "        # Remove the entries (i.e. dictionaries) that contain\n",
    "        # the key 'newest_id' from the payload, i.e. the result \n",
    "        # of our query (which is a list of dictionaries).        \n",
    "        clean_json_list = [x for x in dictLS if 'newest_id' not in x]        \n",
    "        \n",
    "        df = pd.json_normalize(clean_json_list)\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def combineWeeks(dataDict):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credentials validated successfully'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods')\n",
    "search1.validate_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1319442612487479301</td>\n",
       "      <td>2020-10-23T00:56:47.000Z</td>\n",
       "      <td>Exactly..they should loose their jobs.. https:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319442601444020225</td>\n",
       "      <td>2020-10-23T00:56:44.000Z</td>\n",
       "      <td>@MeidasTouch Is Trump going schizo on us that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319442291749158915</td>\n",
       "      <td>2020-10-23T00:55:31.000Z</td>\n",
       "      <td>Thank you @connectmeetings for getting meeting...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1319442241052680193</td>\n",
       "      <td>2020-10-23T00:55:18.000Z</td>\n",
       "      <td>@jecoreyarthur Or another option for jobs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1319442109917728770</td>\n",
       "      <td>2020-10-23T00:54:47.000Z</td>\n",
       "      <td>“They took our jobs!!” Bro u didnt go to colle...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1319442103164936193</td>\n",
       "      <td>2020-10-23T00:54:46.000Z</td>\n",
       "      <td>I'm horrified by this. Any health professional...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1319441826332409856</td>\n",
       "      <td>2020-10-23T00:53:40.000Z</td>\n",
       "      <td>Part of me says “if only Hunter didn’t take th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1319441463671967746</td>\n",
       "      <td>2020-10-23T00:52:13.000Z</td>\n",
       "      <td>@JohnDiesattheEn Modern debates are the NFL Bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1319441090911547392</td>\n",
       "      <td>2020-10-23T00:50:44.000Z</td>\n",
       "      <td>You have?\\nThat’s all I want your amazing Earl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1319440777085419521</td>\n",
       "      <td>2020-10-23T00:49:29.000Z</td>\n",
       "      <td>I’d say that this is a reason some jobs can’t ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1319440647552684034</td>\n",
       "      <td>2020-10-23T00:48:59.000Z</td>\n",
       "      <td>@WriterDf Only that it has never happened in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1319439921212514305</td>\n",
       "      <td>2020-10-23T00:46:05.000Z</td>\n",
       "      <td>Interested in a #FireSafety or #PublicSafety c...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1319439749191520256</td>\n",
       "      <td>2020-10-23T00:45:24.000Z</td>\n",
       "      <td>@ADub1581 @WillBrinson I legitimately think Be...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1319439533189070848</td>\n",
       "      <td>2020-10-23T00:44:33.000Z</td>\n",
       "      <td>working in retail I feel could be one of the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1319438857390247937</td>\n",
       "      <td>2020-10-23T00:41:52.000Z</td>\n",
       "      <td>@AirAssets @BShipspotting @WarshipCam @Plymout...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1319438828499881986</td>\n",
       "      <td>2020-10-23T00:41:45.000Z</td>\n",
       "      <td>@realDonaldTrump Mr. President doing a great j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1319438356330115072</td>\n",
       "      <td>2020-10-23T00:39:52.000Z</td>\n",
       "      <td>@KwikWarren And that’s why you don’t see him w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1319437588999012352</td>\n",
       "      <td>2020-10-23T00:36:49.000Z</td>\n",
       "      <td>@TroyAikman @Buck the two of you need to shut ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1319437496342642688</td>\n",
       "      <td>2020-10-23T00:36:27.000Z</td>\n",
       "      <td>Also, consider though that people were making ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1319437375202684928</td>\n",
       "      <td>2020-10-23T00:35:58.000Z</td>\n",
       "      <td>@WhiteHouse He’s the first person to kill 222,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1319437260236861446</td>\n",
       "      <td>2020-10-23T00:35:31.000Z</td>\n",
       "      <td>2020 in a nutshell: we have more faith in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1319436072351301633</td>\n",
       "      <td>2020-10-23T00:30:48.000Z</td>\n",
       "      <td>@jd1974815 @wendyp4545 @ChuckCallesto @tickels...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1319435924971859969</td>\n",
       "      <td>2020-10-23T00:30:13.000Z</td>\n",
       "      <td>#VoteBlue Democrats have way more good paying ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1319435615805427712</td>\n",
       "      <td>2020-10-23T00:28:59.000Z</td>\n",
       "      <td>No one cares about Hunter Biden  !!! Get real ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1319435474671292416</td>\n",
       "      <td>2020-10-23T00:28:25.000Z</td>\n",
       "      <td>Honestly I don’t understand this. Mass and I m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1319435390265208834</td>\n",
       "      <td>2020-10-23T00:28:05.000Z</td>\n",
       "      <td>@MeghanMcCain He’s still a traitor! He’s made ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1319434320248012804</td>\n",
       "      <td>2020-10-23T00:23:50.000Z</td>\n",
       "      <td>Instead of talking about hoax's and ramming ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1319433699667050497</td>\n",
       "      <td>2020-10-23T00:21:22.000Z</td>\n",
       "      <td>Put my 2 weeks in at one of my jobs today. I’m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1319432261801021440</td>\n",
       "      <td>2020-10-23T00:15:39.000Z</td>\n",
       "      <td>Firefighting resources are stretched thin. @ri...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1319432051867738113</td>\n",
       "      <td>2020-10-23T00:14:49.000Z</td>\n",
       "      <td>Worked 55 hours this week from both jobs. I am...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1319431939565129728</td>\n",
       "      <td>2020-10-23T00:14:22.000Z</td>\n",
       "      <td>@reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1319431911580774400</td>\n",
       "      <td>2020-10-23T00:14:16.000Z</td>\n",
       "      <td>Nancy is a disgrace how she dare to put raises...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1319431528603127808</td>\n",
       "      <td>2020-10-23T00:12:44.000Z</td>\n",
       "      <td>So uhm why can’t jobs just be like ‘yea so we ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1319430987428761600</td>\n",
       "      <td>2020-10-23T00:10:35.000Z</td>\n",
       "      <td>No person on this planet would give Trump a nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1319430894134976519</td>\n",
       "      <td>2020-10-23T00:10:13.000Z</td>\n",
       "      <td>Admitting that a deadly pandemic exists, that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1319429909429760001</td>\n",
       "      <td>2020-10-23T00:06:18.000Z</td>\n",
       "      <td>@yasminv @kwelkernbc Triple threats, beautiful...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1319429724117061633</td>\n",
       "      <td>2020-10-23T00:05:34.000Z</td>\n",
       "      <td>This is the worst PR for iPhone since Steve Jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1319429167167860736</td>\n",
       "      <td>2020-10-23T00:03:21.000Z</td>\n",
       "      <td>You know we have a healthy democracy when sayi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1319429110033076224</td>\n",
       "      <td>2020-10-23T00:03:08.000Z</td>\n",
       "      <td>@agamemnus_dev @FirenzeMike @realDonaldTrump 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1319428993288863744</td>\n",
       "      <td>2020-10-23T00:02:40.000Z</td>\n",
       "      <td>Join the CVS Health team! See our latest job o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                created_at  \\\n",
       "0   1319442612487479301  2020-10-23T00:56:47.000Z   \n",
       "1   1319442601444020225  2020-10-23T00:56:44.000Z   \n",
       "2   1319442291749158915  2020-10-23T00:55:31.000Z   \n",
       "3   1319442241052680193  2020-10-23T00:55:18.000Z   \n",
       "4   1319442109917728770  2020-10-23T00:54:47.000Z   \n",
       "5   1319442103164936193  2020-10-23T00:54:46.000Z   \n",
       "6   1319441826332409856  2020-10-23T00:53:40.000Z   \n",
       "7   1319441463671967746  2020-10-23T00:52:13.000Z   \n",
       "8   1319441090911547392  2020-10-23T00:50:44.000Z   \n",
       "9   1319440777085419521  2020-10-23T00:49:29.000Z   \n",
       "10  1319440647552684034  2020-10-23T00:48:59.000Z   \n",
       "11  1319439921212514305  2020-10-23T00:46:05.000Z   \n",
       "12  1319439749191520256  2020-10-23T00:45:24.000Z   \n",
       "13  1319439533189070848  2020-10-23T00:44:33.000Z   \n",
       "14  1319438857390247937  2020-10-23T00:41:52.000Z   \n",
       "15  1319438828499881986  2020-10-23T00:41:45.000Z   \n",
       "16  1319438356330115072  2020-10-23T00:39:52.000Z   \n",
       "17  1319437588999012352  2020-10-23T00:36:49.000Z   \n",
       "18  1319437496342642688  2020-10-23T00:36:27.000Z   \n",
       "19  1319437375202684928  2020-10-23T00:35:58.000Z   \n",
       "20  1319437260236861446  2020-10-23T00:35:31.000Z   \n",
       "21  1319436072351301633  2020-10-23T00:30:48.000Z   \n",
       "22  1319435924971859969  2020-10-23T00:30:13.000Z   \n",
       "23  1319435615805427712  2020-10-23T00:28:59.000Z   \n",
       "24  1319435474671292416  2020-10-23T00:28:25.000Z   \n",
       "25  1319435390265208834  2020-10-23T00:28:05.000Z   \n",
       "26  1319434320248012804  2020-10-23T00:23:50.000Z   \n",
       "27  1319433699667050497  2020-10-23T00:21:22.000Z   \n",
       "28  1319432261801021440  2020-10-23T00:15:39.000Z   \n",
       "29  1319432051867738113  2020-10-23T00:14:49.000Z   \n",
       "30  1319431939565129728  2020-10-23T00:14:22.000Z   \n",
       "31  1319431911580774400  2020-10-23T00:14:16.000Z   \n",
       "32  1319431528603127808  2020-10-23T00:12:44.000Z   \n",
       "33  1319430987428761600  2020-10-23T00:10:35.000Z   \n",
       "34  1319430894134976519  2020-10-23T00:10:13.000Z   \n",
       "35  1319429909429760001  2020-10-23T00:06:18.000Z   \n",
       "36  1319429724117061633  2020-10-23T00:05:34.000Z   \n",
       "37  1319429167167860736  2020-10-23T00:03:21.000Z   \n",
       "38  1319429110033076224  2020-10-23T00:03:08.000Z   \n",
       "39  1319428993288863744  2020-10-23T00:02:40.000Z   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Exactly..they should loose their jobs.. https:...   \n",
       "1   @MeidasTouch Is Trump going schizo on us that ...   \n",
       "2   Thank you @connectmeetings for getting meeting...   \n",
       "3           @jecoreyarthur Or another option for jobs   \n",
       "4   “They took our jobs!!” Bro u didnt go to colle...   \n",
       "5   I'm horrified by this. Any health professional...   \n",
       "6   Part of me says “if only Hunter didn’t take th...   \n",
       "7   @JohnDiesattheEn Modern debates are the NFL Bl...   \n",
       "8   You have?\\nThat’s all I want your amazing Earl...   \n",
       "9   I’d say that this is a reason some jobs can’t ...   \n",
       "10  @WriterDf Only that it has never happened in t...   \n",
       "11  Interested in a #FireSafety or #PublicSafety c...   \n",
       "12  @ADub1581 @WillBrinson I legitimately think Be...   \n",
       "13  working in retail I feel could be one of the m...   \n",
       "14  @AirAssets @BShipspotting @WarshipCam @Plymout...   \n",
       "15  @realDonaldTrump Mr. President doing a great j...   \n",
       "16  @KwikWarren And that’s why you don’t see him w...   \n",
       "17  @TroyAikman @Buck the two of you need to shut ...   \n",
       "18  Also, consider though that people were making ...   \n",
       "19  @WhiteHouse He’s the first person to kill 222,...   \n",
       "20  2020 in a nutshell: we have more faith in the ...   \n",
       "21  @jd1974815 @wendyp4545 @ChuckCallesto @tickels...   \n",
       "22  #VoteBlue Democrats have way more good paying ...   \n",
       "23  No one cares about Hunter Biden  !!! Get real ...   \n",
       "24  Honestly I don’t understand this. Mass and I m...   \n",
       "25  @MeghanMcCain He’s still a traitor! He’s made ...   \n",
       "26  Instead of talking about hoax's and ramming ju...   \n",
       "27  Put my 2 weeks in at one of my jobs today. I’m...   \n",
       "28  Firefighting resources are stretched thin. @ri...   \n",
       "29  Worked 55 hours this week from both jobs. I am...   \n",
       "30  @reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...   \n",
       "31  Nancy is a disgrace how she dare to put raises...   \n",
       "32  So uhm why can’t jobs just be like ‘yea so we ...   \n",
       "33  No person on this planet would give Trump a nu...   \n",
       "34  Admitting that a deadly pandemic exists, that ...   \n",
       "35  @yasminv @kwelkernbc Triple threats, beautiful...   \n",
       "36  This is the worst PR for iPhone since Steve Jo...   \n",
       "37  You know we have a healthy democracy when sayi...   \n",
       "38  @agamemnus_dev @FirenzeMike @realDonaldTrump 1...   \n",
       "39  Join the CVS Health team! See our latest job o...   \n",
       "\n",
       "    public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                              0                           0   \n",
       "1                              1                           1   \n",
       "2                              1                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           0   \n",
       "5                              0                           0   \n",
       "6                              0                           0   \n",
       "7                              0                           0   \n",
       "8                              0                           0   \n",
       "9                              0                           1   \n",
       "10                             0                           1   \n",
       "11                             3                           1   \n",
       "12                             0                           1   \n",
       "13                             0                           0   \n",
       "14                             0                           1   \n",
       "15                             0                           0   \n",
       "16                             0                           1   \n",
       "17                             0                           0   \n",
       "18                             0                           2   \n",
       "19                             1                           0   \n",
       "20                             0                           0   \n",
       "21                             0                           0   \n",
       "22                             0                           0   \n",
       "23                             1                           0   \n",
       "24                             0                           0   \n",
       "25                             0                           0   \n",
       "26                             0                           2   \n",
       "27                             0                           1   \n",
       "28                            12                           2   \n",
       "29                             0                           0   \n",
       "30                             0                           0   \n",
       "31                             1                           0   \n",
       "32                             0                           0   \n",
       "33                             0                           0   \n",
       "34                             0                           0   \n",
       "35                             0                           0   \n",
       "36                             0                           0   \n",
       "37                             0                           0   \n",
       "38                             0                           0   \n",
       "39                             1                           0   \n",
       "\n",
       "    public_metrics.like_count  public_metrics.quote_count  \n",
       "0                           0                           0  \n",
       "1                           3                           0  \n",
       "2                           3                           2  \n",
       "3                           0                           0  \n",
       "4                           0                           0  \n",
       "5                           1                           0  \n",
       "6                           2                           0  \n",
       "7                           0                           0  \n",
       "8                           0                           0  \n",
       "9                           2                           0  \n",
       "10                          0                           0  \n",
       "11                          3                           0  \n",
       "12                         11                           1  \n",
       "13                          1                           0  \n",
       "14                          1                           0  \n",
       "15                          0                           0  \n",
       "16                          2                           0  \n",
       "17                          0                           0  \n",
       "18                          1                           0  \n",
       "19                          2                           0  \n",
       "20                          0                           0  \n",
       "21                          0                           0  \n",
       "22                          0                           0  \n",
       "23                          0                           0  \n",
       "24                          0                           0  \n",
       "25                          0                           0  \n",
       "26                          4                           0  \n",
       "27                          1                           0  \n",
       "28                         18                           3  \n",
       "29                          0                           0  \n",
       "30                          1                           0  \n",
       "31                          0                           0  \n",
       "32                          0                           0  \n",
       "33                          0                           0  \n",
       "34                          4                           0  \n",
       "35                          0                           0  \n",
       "36                          0                           0  \n",
       "37                          1                           0  \n",
       "38                          0                           0  \n",
       "39                          0                           0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a query.\n",
    "search1.build_query('jobs','2020-10-23 00:00', '2020-10-23 01:00', results_per_call=500)\n",
    "\n",
    "# Getting payload. \n",
    "# This is saved in self.results.\n",
    "search1.get_data(nTweets = 1000)\n",
    "\n",
    "# Clean data and save in a pd.DataFrame\n",
    "df1 = twitterData.get_df(search1.result)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of requests in a single query is saved in the instance attributed .rs.n_requests. \n",
    "* This is overwritten when a new request is made, but before that, this number (n_request) is added to the instance's .all_requests attribute. \n",
    "    * For example, below we can see that .n_requests = 1 after both the first and second payload (saved in df1), but .all_requests is 3. \n",
    "* The .all_requests attribute will be used for ensurign compliance with rate limits.\n",
    "    * This could be done directly through *searchtweets*, which has built-in tools (e.g. exponential back-off), by making a single query for the whole period (~280 days).\n",
    "    * However, since the period we are interested in covering here is quite big, this is probably not a good solutions (e.g. if something fails on request 5,000/7,000 all data is lost but all tweets already accessed will count towards the monthly rate limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "17\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-23T01:59:41.000Z</td>\n",
       "      <td>Many of the jobs lost this year will never com...</td>\n",
       "      <td>1319458440633339904</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-23T01:59:40.000Z</td>\n",
       "      <td>I have a 401K and investments. I like seeing t...</td>\n",
       "      <td>1319458438943019009</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-23T01:58:56.000Z</td>\n",
       "      <td>You’re all sitting here while you’re still all...</td>\n",
       "      <td>1319458252460085249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-23T01:58:43.000Z</td>\n",
       "      <td>In case you didn’t know this, the stock market...</td>\n",
       "      <td>1319458198932381697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-23T01:58:15.000Z</td>\n",
       "      <td>@Jaycaleb8 Bro what are you saying, I live dow...</td>\n",
       "      <td>1319458079759568896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2020-10-23T01:05:01.000Z</td>\n",
       "      <td>Looking forward to hearing @JoeBiden talk abou...</td>\n",
       "      <td>1319444682720661504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2020-10-23T01:03:45.000Z</td>\n",
       "      <td>If Biden thinks Trump mishandled Covid 19 he n...</td>\n",
       "      <td>1319444365014609920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2020-10-23T01:02:48.000Z</td>\n",
       "      <td>@rrt003 @MSNBC @DailyNewsSA Well no. Also how ...</td>\n",
       "      <td>1319444125972914176</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2020-10-23T01:02:16.000Z</td>\n",
       "      <td>Gays dont not liking Trump as a person. Ooohh ...</td>\n",
       "      <td>1319443992451248129</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2020-10-23T01:01:32.000Z</td>\n",
       "      <td>@LindseyGrahamSC All economists say @JoeBiden ...</td>\n",
       "      <td>1319443807641825280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "0   2020-10-23T01:59:41.000Z   \n",
       "1   2020-10-23T01:59:40.000Z   \n",
       "2   2020-10-23T01:58:56.000Z   \n",
       "3   2020-10-23T01:58:43.000Z   \n",
       "4   2020-10-23T01:58:15.000Z   \n",
       "..                       ...   \n",
       "59  2020-10-23T01:05:01.000Z   \n",
       "60  2020-10-23T01:03:45.000Z   \n",
       "61  2020-10-23T01:02:48.000Z   \n",
       "62  2020-10-23T01:02:16.000Z   \n",
       "63  2020-10-23T01:01:32.000Z   \n",
       "\n",
       "                                                 text                   id  \\\n",
       "0   Many of the jobs lost this year will never com...  1319458440633339904   \n",
       "1   I have a 401K and investments. I like seeing t...  1319458438943019009   \n",
       "2   You’re all sitting here while you’re still all...  1319458252460085249   \n",
       "3   In case you didn’t know this, the stock market...  1319458198932381697   \n",
       "4   @Jaycaleb8 Bro what are you saying, I live dow...  1319458079759568896   \n",
       "..                                                ...                  ...   \n",
       "59  Looking forward to hearing @JoeBiden talk abou...  1319444682720661504   \n",
       "60  If Biden thinks Trump mishandled Covid 19 he n...  1319444365014609920   \n",
       "61  @rrt003 @MSNBC @DailyNewsSA Well no. Also how ...  1319444125972914176   \n",
       "62  Gays dont not liking Trump as a person. Ooohh ...  1319443992451248129   \n",
       "63  @LindseyGrahamSC All economists say @JoeBiden ...  1319443807641825280   \n",
       "\n",
       "    public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                              0                           1   \n",
       "1                              0                           1   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           1   \n",
       "..                           ...                         ...   \n",
       "59                             0                           0   \n",
       "60                             0                           0   \n",
       "61                             0                           1   \n",
       "62                             2                           4   \n",
       "63                             0                           0   \n",
       "\n",
       "    public_metrics.like_count  public_metrics.quote_count  \n",
       "0                           2                           0  \n",
       "1                           1                           0  \n",
       "2                           0                           0  \n",
       "3                           1                           0  \n",
       "4                           0                           0  \n",
       "..                        ...                         ...  \n",
       "59                          2                           0  \n",
       "60                          0                           0  \n",
       "61                          1                           0  \n",
       "62                         40                           1  \n",
       "63                          0                           0  \n",
       "\n",
       "[64 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1.build_query('jobs','2020-10-23 01:00', '2020-10-23 2:00', results_per_call=500)\n",
    "search1.get_data(nTweets = 1000)\n",
    "df2 = twitterData.get_df(search1.result)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "55\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above also gives an indication of how many tweets to expect for our most basic query on 'jobs' (in one hour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 week based on the data collection periods in the surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey periods\n",
    "# These hold the dates on which data was collected for each survey in part\n",
    "# Will use it to get twitter data from the same period.\n",
    "surveyPeriods = pd.read_excel('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Surveys/table_details/surveyPeriods.xlsx', sheet_name='AI+HPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week start(Monday): 2020-10-19 \n",
      "Week end (Sunday): 2020-10-25\n",
      "Var type: <class 'str'>,<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "p1_start, p2_start = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "print(\"Week start(Monday): {} \\nWeek end (Sunday): {}\\nVar type: {},{}\".format(p1_start, p2_start, type(p1_start), type(p2_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1320151587403288579</td>\n",
       "      <td>2020-10-24T23:54:00.000Z</td>\n",
       "      <td>The need to pick up the pace on transitioning ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1320151419857498112</td>\n",
       "      <td>2020-10-24T23:53:20.000Z</td>\n",
       "      <td>We're hiring! Click to apply: Restaurant Manag...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1320151277926555660</td>\n",
       "      <td>2020-10-24T23:52:46.000Z</td>\n",
       "      <td>‘Even on a rainy day — you can tell the Sunshi...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1320151038448390144</td>\n",
       "      <td>2020-10-24T23:51:49.000Z</td>\n",
       "      <td>Life is a struggle especially in between jobs ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1320150725825941504</td>\n",
       "      <td>2020-10-24T23:50:34.000Z</td>\n",
       "      <td>💯 New York remains desirable for all the same ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1319303811517812737</td>\n",
       "      <td>2020-10-22T15:45:14.000Z</td>\n",
       "      <td>@MikeEmanuelFox @TeamTrump @CNN @MSNBC @ABC @C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>1319303678533259269</td>\n",
       "      <td>2020-10-22T15:44:43.000Z</td>\n",
       "      <td>New claims below 800k for first time in foreve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>1319303241998520322</td>\n",
       "      <td>2020-10-22T15:42:59.000Z</td>\n",
       "      <td>@TrumpWarRoom @CNN @MSNBC @ABC @CBS @NBCNews a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>1319303155746787328</td>\n",
       "      <td>2020-10-22T15:42:38.000Z</td>\n",
       "      <td>@JoeBiden @CNN @MSNBC @ABC @CBS @NBCNews are y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>1319303065778987008</td>\n",
       "      <td>2020-10-22T15:42:17.000Z</td>\n",
       "      <td>@jpalmiotti Looking forward to trying this. Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                created_at  \\\n",
       "0     1320151587403288579  2020-10-24T23:54:00.000Z   \n",
       "1     1320151419857498112  2020-10-24T23:53:20.000Z   \n",
       "2     1320151277926555660  2020-10-24T23:52:46.000Z   \n",
       "3     1320151038448390144  2020-10-24T23:51:49.000Z   \n",
       "4     1320150725825941504  2020-10-24T23:50:34.000Z   \n",
       "...                   ...                       ...   \n",
       "2438  1319303811517812737  2020-10-22T15:45:14.000Z   \n",
       "2439  1319303678533259269  2020-10-22T15:44:43.000Z   \n",
       "2440  1319303241998520322  2020-10-22T15:42:59.000Z   \n",
       "2441  1319303155746787328  2020-10-22T15:42:38.000Z   \n",
       "2442  1319303065778987008  2020-10-22T15:42:17.000Z   \n",
       "\n",
       "                                                   text  \\\n",
       "0     The need to pick up the pace on transitioning ...   \n",
       "1     We're hiring! Click to apply: Restaurant Manag...   \n",
       "2     ‘Even on a rainy day — you can tell the Sunshi...   \n",
       "3     Life is a struggle especially in between jobs ...   \n",
       "4     💯 New York remains desirable for all the same ...   \n",
       "...                                                 ...   \n",
       "2438  @MikeEmanuelFox @TeamTrump @CNN @MSNBC @ABC @C...   \n",
       "2439  New claims below 800k for first time in foreve...   \n",
       "2440  @TrumpWarRoom @CNN @MSNBC @ABC @CBS @NBCNews a...   \n",
       "2441  @JoeBiden @CNN @MSNBC @ABC @CBS @NBCNews are y...   \n",
       "2442  @jpalmiotti Looking forward to trying this. Ma...   \n",
       "\n",
       "      public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                                8                           0   \n",
       "1                                0                           0   \n",
       "2                                2                           1   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "2438                             0                           0   \n",
       "2439                             1                           0   \n",
       "2440                             0                           0   \n",
       "2441                             0                           0   \n",
       "2442                             0                           0   \n",
       "\n",
       "      public_metrics.like_count  public_metrics.quote_count  \n",
       "0                            34                           0  \n",
       "1                             0                           0  \n",
       "2                             7                           0  \n",
       "3                             0                           0  \n",
       "4                             4                           0  \n",
       "...                         ...                         ...  \n",
       "2438                          0                           0  \n",
       "2439                          0                           0  \n",
       "2440                          0                           0  \n",
       "2441                          0                           0  \n",
       "2442                          3                           0  \n",
       "\n",
       "[2443 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods')\n",
    "search2.validate_credentials()\n",
    "search2.build_query('jobs', p1_start, p2_start)\n",
    "search2.get_data(nTweets=20000) # Covering a whole week now -> higher nTweets set as the limit.\n",
    "df3 = twitterData.get_df(search2.result)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2.rs.session_request_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the weeks from survey data collection periods.\n",
    "\n",
    "This will be moved to the surveyDates method of twitterData. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single week start and end date as tuple. \n",
    "bla = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "\n",
    "# Get all the week's start and end date as a list of tuples.\n",
    "bla2 = [twitterData.weekFromDay(date) for date in surveyPeriods['A_I_start_date']]\n",
    "\n",
    "# Get the Monday dates.\n",
    "bla3 = [bla2[i][0] for i in range(len(bla2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This will be used later on for getting the overlap periods. \n",
    "* Howver, what we actuall want here is all the the week start and end dates from the first A/I collection period to the last (with no breaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first monday and last sunday from the A/I data collection periods\n",
    "firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "_, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "# Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "\n",
    "# Saving results in a tuple\n",
    "bla4 = (mondays[0].strftime('%Y-%m-%d'), sundays[0].strftime('%Y-%m-%d'))\n",
    "\n",
    "# Saving all the results in a tuple\n",
    "bla5 = [(m.strftime('%Y-%m-%d'), s.strftime('%Y-%m-%d')) for m, s in zip(mondays, sundays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22'),\n",
       " ('2020-11-30', '2020-12-06'),\n",
       " ('2020-12-07', '2020-12-13'),\n",
       " ('2020-12-14', '2020-12-20'),\n",
       " ('2021-01-04', '2021-01-10'),\n",
       " ('2021-01-18', '2021-01-24'),\n",
       " ('2021-01-25', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-07'),\n",
       " ('2021-02-15', '2021-02-21'),\n",
       " ('2021-02-22', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-07'),\n",
       " ('2021-03-15', '2021-03-21'),\n",
       " ('2021-03-29', '2021-04-04'),\n",
       " ('2021-04-12', '2021-04-18'),\n",
       " ('2021-05-03', '2021-05-09'),\n",
       " ('2021-05-17', '2021-05-23')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-10-26', '2020-11-01'),\n",
       " ('2020-11-02', '2020-11-08'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22'),\n",
       " ('2020-11-23', '2020-11-29'),\n",
       " ('2020-11-30', '2020-12-06'),\n",
       " ('2020-12-07', '2020-12-13'),\n",
       " ('2020-12-14', '2020-12-20'),\n",
       " ('2020-12-21', '2020-12-27'),\n",
       " ('2020-12-28', '2021-01-03'),\n",
       " ('2021-01-04', '2021-01-10'),\n",
       " ('2021-01-11', '2021-01-17'),\n",
       " ('2021-01-18', '2021-01-24'),\n",
       " ('2021-01-25', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-07'),\n",
       " ('2021-02-08', '2021-02-14'),\n",
       " ('2021-02-15', '2021-02-21'),\n",
       " ('2021-02-22', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-07'),\n",
       " ('2021-03-08', '2021-03-14'),\n",
       " ('2021-03-15', '2021-03-21'),\n",
       " ('2021-03-22', '2021-03-28'),\n",
       " ('2021-03-29', '2021-04-04'),\n",
       " ('2021-04-05', '2021-04-11'),\n",
       " ('2021-04-12', '2021-04-18'),\n",
       " ('2021-04-19', '2021-04-25'),\n",
       " ('2021-04-26', '2021-05-02'),\n",
       " ('2021-05-03', '2021-05-09'),\n",
       " ('2021-05-10', '2021-05-16'),\n",
       " ('2021-05-17', '2021-05-23')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated version of the above.\n",
    "\n",
    "# Get first monday and last sunday from the A/I data collection periods\n",
    "firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "_, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "# Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "\n",
    "# Get strings\n",
    "mondays_str = mondays.strftime('%Y-%m-%d')\n",
    "sundays_str = sundays.strftime('%Y-%m-%d')\n",
    "\n",
    "# Saving all the results in a tuple\n",
    "all_weeks = [(m, s) for m, s in zip(mondays, sundays)]\n",
    "all_weeks_str = [(m, s) for m, s in zip(mondays_str, sundays_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2020-10-19 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-10-25 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-10-26 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-01 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-02 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-08 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-09 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-15 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-16 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-22 00:00:00', freq='W-SUN'))]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-10-26', '2020-11-01'),\n",
       " ('2020-11-02', '2020-11-08'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks_str[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020-10-19', '2020-10-26', '2020-11-02', '2020-11-09', '2020-11-16',\n",
       "       '2020-11-23', '2020-11-30', '2020-12-07', '2020-12-14', '2020-12-21',\n",
       "       '2020-12-28', '2021-01-04', '2021-01-11', '2021-01-18', '2021-01-25',\n",
       "       '2021-02-01', '2021-02-08', '2021-02-15', '2021-02-22', '2021-03-01',\n",
       "       '2021-03-08', '2021-03-15', '2021-03-22', '2021-03-29', '2021-04-05',\n",
       "       '2021-04-12', '2021-04-19', '2021-04-26', '2021-05-03', '2021-05-10',\n",
       "       '2021-05-17'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondays_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRECTION: Building the weeks from survey data collection periods.\n",
    "\n",
    "The endate used by the API actually refers to the previous day until 23:59.\n",
    "* e.g. '2020-10-26' will search everything up until '2020-10-25 23:59'\n",
    "* So *sundays* are not actually needed, the endDate in each week's query will just be the following monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version of the above.\n",
    "\n",
    "# @staticmethod\n",
    "def nextMonday(date):\n",
    "    date = twitterData.toDatetime(date)\n",
    "    \n",
    "    nextM = date + timedelta(days=-date.weekday(), weeks=1)\n",
    "    \n",
    "    return nextM\n",
    "    \n",
    "\n",
    "# Get first monday and last sunday from the A/I data collection periods\n",
    "firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "_, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "# Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "leading_mondays = pd.date_range(nextMonday(firstDate), nextMonday(lastDate), freq='W-MON')\n",
    "\n",
    "# Get strings\n",
    "mondays_str = mondays.strftime('%Y-%m-%d')\n",
    "leading_mondays = leading_mondays.strftime('%Y-%m-%d')\n",
    "\n",
    "# Saving all the results in a tuple\n",
    "all_weeks = [(m, s) for m, s in zip(mondays, leading_mondays)]\n",
    "all_weeks_str = [(m, s) for m, s in zip(mondays_str, leading_mondays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-26'),\n",
       " ('2020-10-26', '2020-11-02'),\n",
       " ('2020-11-02', '2020-11-09'),\n",
       " ('2020-11-09', '2020-11-16'),\n",
       " ('2020-11-16', '2020-11-23'),\n",
       " ('2020-11-23', '2020-11-30'),\n",
       " ('2020-11-30', '2020-12-07'),\n",
       " ('2020-12-07', '2020-12-14'),\n",
       " ('2020-12-14', '2020-12-21'),\n",
       " ('2020-12-21', '2020-12-28'),\n",
       " ('2020-12-28', '2021-01-04'),\n",
       " ('2021-01-04', '2021-01-11'),\n",
       " ('2021-01-11', '2021-01-18'),\n",
       " ('2021-01-18', '2021-01-25'),\n",
       " ('2021-01-25', '2021-02-01'),\n",
       " ('2021-02-01', '2021-02-08'),\n",
       " ('2021-02-08', '2021-02-15'),\n",
       " ('2021-02-15', '2021-02-22'),\n",
       " ('2021-02-22', '2021-03-01'),\n",
       " ('2021-03-01', '2021-03-08'),\n",
       " ('2021-03-08', '2021-03-15'),\n",
       " ('2021-03-15', '2021-03-22'),\n",
       " ('2021-03-22', '2021-03-29'),\n",
       " ('2021-03-29', '2021-04-05'),\n",
       " ('2021-04-05', '2021-04-12'),\n",
       " ('2021-04-12', '2021-04-19'),\n",
       " ('2021-04-19', '2021-04-26'),\n",
       " ('2021-04-26', '2021-05-03'),\n",
       " ('2021-05-03', '2021-05-10'),\n",
       " ('2021-05-10', '2021-05-17'),\n",
       " ('2021-05-17', '2021-05-24')]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "bla = twitterData.toDatetime(lastDate)\n",
    "\n",
    "following_monday = bla + timedelta(days=-bla.weekday(), weeks=1)\n",
    "print(following_monday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-10-19', '2020-10-26', '2020-11-02', '2020-11-09',\n",
       "               '2020-11-16', '2020-11-23', '2020-11-30', '2020-12-07',\n",
       "               '2020-12-14', '2020-12-21', '2020-12-28', '2021-01-04',\n",
       "               '2021-01-11', '2021-01-18', '2021-01-25', '2021-02-01',\n",
       "               '2021-02-08', '2021-02-15', '2021-02-22', '2021-03-01',\n",
       "               '2021-03-08', '2021-03-15', '2021-03-22', '2021-03-29',\n",
       "               '2021-04-05', '2021-04-12', '2021-04-19', '2021-04-26',\n",
       "               '2021-05-03', '2021-05-10', '2021-05-17'],\n",
       "              dtype='datetime64[ns]', freq='W-MON')"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: using the oneWeek() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-19\n",
      "2020-10-26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1320515328955293703</td>\n",
       "      <td>@Gregt041 @SeanHouse90 @WPTV Let me get this s...</td>\n",
       "      <td>2020-10-25T23:59:23.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1320515190572666882</td>\n",
       "      <td>@patrickbetdavid China/Jobs</td>\n",
       "      <td>2020-10-25T23:58:50.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1320514772975132672</td>\n",
       "      <td>How I did my last fore jobs these 6 months wit...</td>\n",
       "      <td>2020-10-25T23:57:10.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1320514318778052608</td>\n",
       "      <td>We’re looking for a talented Restaurant Manage...</td>\n",
       "      <td>2020-10-25T23:55:22.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1320513842644979712</td>\n",
       "      <td>Was @GovWhitmer in on purchase of @HennigesCon...</td>\n",
       "      <td>2020-10-25T23:53:28.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>1317981967376207873</td>\n",
       "      <td>@realDonaldTrump You are President we have los...</td>\n",
       "      <td>2020-10-19T00:12:42.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>1317981499208142849</td>\n",
       "      <td>@AbjornSell @StacyLStiles @JoeBiden Obama gove...</td>\n",
       "      <td>2020-10-19T00:10:50.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>1317980815842750466</td>\n",
       "      <td>@KellyO How desperate are \"reporters\" to save ...</td>\n",
       "      <td>2020-10-19T00:08:08.000Z</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>1317979592125276162</td>\n",
       "      <td>So many things wrong with this 4 min clip. \\n\\...</td>\n",
       "      <td>2020-10-19T00:03:16.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>1317979297886269441</td>\n",
       "      <td>Deregulation. He has deregulated polluters and...</td>\n",
       "      <td>2020-10-19T00:02:06.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5527 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0     1320515328955293703  @Gregt041 @SeanHouse90 @WPTV Let me get this s...   \n",
       "1     1320515190572666882                        @patrickbetdavid China/Jobs   \n",
       "2     1320514772975132672  How I did my last fore jobs these 6 months wit...   \n",
       "3     1320514318778052608  We’re looking for a talented Restaurant Manage...   \n",
       "4     1320513842644979712  Was @GovWhitmer in on purchase of @HennigesCon...   \n",
       "...                   ...                                                ...   \n",
       "5522  1317981967376207873  @realDonaldTrump You are President we have los...   \n",
       "5523  1317981499208142849  @AbjornSell @StacyLStiles @JoeBiden Obama gove...   \n",
       "5524  1317980815842750466  @KellyO How desperate are \"reporters\" to save ...   \n",
       "5525  1317979592125276162  So many things wrong with this 4 min clip. \\n\\...   \n",
       "5526  1317979297886269441  Deregulation. He has deregulated polluters and...   \n",
       "\n",
       "                    created_at  public_metrics.retweet_count  \\\n",
       "0     2020-10-25T23:59:23.000Z                             0   \n",
       "1     2020-10-25T23:58:50.000Z                             0   \n",
       "2     2020-10-25T23:57:10.000Z                             0   \n",
       "3     2020-10-25T23:55:22.000Z                             1   \n",
       "4     2020-10-25T23:53:28.000Z                             0   \n",
       "...                        ...                           ...   \n",
       "5522  2020-10-19T00:12:42.000Z                             0   \n",
       "5523  2020-10-19T00:10:50.000Z                             0   \n",
       "5524  2020-10-19T00:08:08.000Z                             8   \n",
       "5525  2020-10-19T00:03:16.000Z                             0   \n",
       "5526  2020-10-19T00:02:06.000Z                             0   \n",
       "\n",
       "      public_metrics.reply_count  public_metrics.like_count  \\\n",
       "0                              2                          0   \n",
       "1                              0                          0   \n",
       "2                              0                          0   \n",
       "3                              0                          0   \n",
       "4                              0                          0   \n",
       "...                          ...                        ...   \n",
       "5522                           0                          0   \n",
       "5523                           0                          0   \n",
       "5524                          11                         43   \n",
       "5525                           0                          0   \n",
       "5526                           1                          0   \n",
       "\n",
       "      public_metrics.quote_count  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "...                          ...  \n",
       "5522                           0  \n",
       "5523                           0  \n",
       "5524                           1  \n",
       "5525                           0  \n",
       "5526                           0  \n",
       "\n",
       "[5527 rows x 7 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search4 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods')\n",
    "search4.validate_credentials()\n",
    "\n",
    "# Generate dates from survey (Axios-Ipsos) info\n",
    "search4.surveyDates()\n",
    "\n",
    "# Create dictionaries for saving the data\n",
    "search4.createDicts()\n",
    "\n",
    "# # Get data from week 1\n",
    "search4.oneWeek('jobs', 1)\n",
    "\n",
    "# # Show dataframe\n",
    "search4.allData[search4.currentWeek]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataframe is the same as what we got with the stacked individual methods.\n",
    "# all(search4.allData[search4.currentWeek] == df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mostRecent': datetime.datetime(2020, 10, 25, 23, 59, 23),\n",
       " 'oldest': datetime.datetime(2020, 10, 19, 0, 2, 6),\n",
       " 'timeCovered': '6 days, 23:57:17',\n",
       " 'sessionRequestCounter': 79,\n",
       " 'totalRequests': 79,\n",
       " 'totalTweets': 5527,\n",
       " 'totalTweetsOverall': 5527,\n",
       " 'requestParams': {'query': '\"jobs\" lang: en place_country:US',\n",
       "  'max_results': 500,\n",
       "  'start_time': '2020-10-19T00:00:00Z',\n",
       "  'end_time': '2020-10-26T00:00:00Z',\n",
       "  'tweet.fields': 'id,created_at,text,public_metrics',\n",
       "  'next_token': 'b26v89c19zqg8o3fos8vq67afna0t42iw3icscqtbqs8t'}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check logs\n",
    "search4.logs[search4.currentWeek]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving\n",
    "search4.exportOneWeek(topic = \"Employment\", saveDF=True, saveJSON=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading\n",
    "testDF, testJSON = twitterData.loadOneWeek(weekStart = '2020-10-19', topic= \"Employment\", loadDF=True, loadJSON=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if these are the same\n",
    "print(all(testDF == search4.allData[search4.currentWeek]))\n",
    "print(testJSON == search4.result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitterSearch",
   "language": "python",
   "name": "twittersearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
