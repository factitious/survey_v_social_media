{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "### Things that need to be done    \n",
    "\n",
    "- [x] Set up skeleton for a nice object oriented approach. \n",
    "- [x] Figure out best way to get tweets.\n",
    "- [x] Data format\n",
    "- [x] Build class and funcs\n",
    "- [x] Figure out how to best count all requests in a session and make sure it's functional\n",
    "    - This is already built-in to searchtweets to some extent, so will use that.\n",
    "- [x] Make oneDay() fail safe (it's a bad idea to get the data and combine it within a single for, because if something goes wrong in an interation all the data from previous iterations will be lost if something goes wrong).\n",
    "    - Getting a whole week instead\n",
    "- [x] Change oneDay() to oneWeek().    \n",
    "- [ ] Save metadata for the payloads (i.e. self.most_recent, self.oldest, self.timeCovered, rs.total_results + other?)\n",
    "    - Seach tweets already saves some sort of log, print this to file with the appropriate name.  \n",
    "\n",
    "### Rate Limits\n",
    "\n",
    "- 10,000,000 Tweets per month (resets on the 19th of each month). \n",
    "- 300 requests/15 minute window, with 500 Tweets/request:\n",
    "    - 150,000 tweets/15min \n",
    "    - 600,000 tweets/hour\n",
    "\n",
    "### How many tweets to get?\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - ~ 280 days\n",
    "    - ~ 6720 hours\n",
    "    - If we get 1000 tweets per hour: $6,720,000 * 2$. \n",
    "    - That's very little in terms of space, but might take quite a while for it to go through sentiment analysis.\n",
    "    - It would take ~22 hours to get the whole data (due to rate limits).\n",
    "    - However, it's unlikely that our queries would return anywhere near 1,000 results/hour.\n",
    "\n",
    "### Best way to get tweets\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - $n_h$ per hour/day\n",
    "    - $n_d$ per day (where $n_d$ would be ~ $n_h*24$)\n",
    "    - $n_w$ per week (where $n_w$ would be ~ $n_h*24 *7$)  \n",
    "    \n",
    "\n",
    "- $n_w$ is probably the best options: \n",
    "    - can leverage functions built into *searchtweets* to avoid rate limit violations (e.g. exponential back-off).\n",
    "    - it's easy to select tweets in any given day/hour from these data.\n",
    "\n",
    "### Data format\n",
    "\n",
    "- A single results call: **JSON to pd**.\n",
    "    - This is relatively straightforward with one minor complication, i.e. entries such as this:\n",
    "    <blockquote>{'newest_id': '1402310241992183808',\n",
    "  'oldest_id': '1402310139630211083',\n",
    "  'result_count': 100,\n",
    "  'next_token': 'b26v89c19zqg8o3fpdg7rbcqdq8stpgmibslekg3kxail'}\n",
    "    </blockquote>\n",
    "    - This is used by the wrapper to get the next lot of tweets if max_tweets > results_per_call, but will also always be the last entry in a result.\n",
    "    \n",
    "    \n",
    "- Multiple result calls: **pds in dict/dict-of-dict**. \n",
    "    - I am thinking the best way to store all the data would be a dict of dataframes, but will see how it works  \n",
    "    \n",
    "### Survey periods\n",
    "\n",
    "| Period | A_I_start_date | A_I_end_date | A_I_week | HPS_start_date | HPS_end_date | HPS_Week | HPS Topic |\n",
    "|--------|----------------|--------------|----------|----------------|--------------|----------|-----------|\n",
    "| P1     | 23.10.2020     | 26.10.2020   | W29*     | 28.10.2020     | 09.11.2020   | W18      |     E    |\n",
    "| P2     | 13.11.2020     | 16.11.2020   | W30      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P2     | 20.11.2020     | 23.11.2020   | W31      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P3     | 04.12.2020     | 07.12.2020   | W32      | 25.11.2020     | 07.12.2020   | W20      |     E    |\n",
    "| P4     | 11.12.2020     | 14.12.2020   | W33      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P4     | 18.12.2020     | 21.12.2020   | W34      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P5     | 08.01.2021     | 11.01.2021   | W35      | 06.01.2021     | 18.01.2021   | W22      |    E,V   |\n",
    "| P6     | 22.01.2021     | 25.01.2021   | W36      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P6     | 29.01.2021     | 01.02.2021   | W37      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P7     | 05.02.2021     | 08.02.2021   | W38      | 03.02.2021     | 15.02.2021   | W24      |    E,V   |\n",
    "| P8     | 19.02.2021     | 22.02.2021   | W39      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P8     | 28.02.2021     | 01.03.2021   | W40      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P9     | 05.03.2021     | 08.03.2021   | W41      | 03.03.2021     | 15.03.2021   | W26      |    E,V   |\n",
    "| P10    | 19.03.2021     | 22.03.2021   | W42      | 17.03.2021     | 29.03.2021   | W27      |    E,V   |\n",
    "| P11    | 02.04.2021     | 05.04.2021   | W43      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P11    | 16.04.2021     | 19.04.2021   | W44      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P12    | 07.05.2021     | 10.05.2021   | W45      | 28.04.2021     | 10.05.2021   | W29      |    E,V   |\n",
    "| P13    | 21.05.2021     | 24.05.2021   | W46      |                |              | W30      |    E,V   |\n",
    "\n",
    "\n",
    "e.g. P1: 23.10.20 - 26.10.20 (Fri - Mon)\n",
    "* Corresponding week 19.19.20 - 25.10.20 or 26.10.20 - 01.11.20?\n",
    "* For now let's say the former. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "from os import path\n",
    "from searchtweets import ResultStream, gen_request_parameters, load_credentials, collect_results, convert_utc_time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2021-05-24 we have 213 days, 5112 hours, and 51120 tweets (with 10 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "def countTweets(startDate, endDate, tweets_per_hour):\n",
    "    '''\n",
    "    Specify dates in DD.MM.YYY format (no leading 0 for months or days)\n",
    "    '''\n",
    "    \n",
    "    s_d, s_m, s_y = [ int(i) for i in startDate.split('.')]\n",
    "    e_d, e_m, e_y = [ int(i) for i in endDate.split('.')]\n",
    "\n",
    "    endDate = date(e_y, e_m, e_d)\n",
    "    startDate = date(s_y, s_m, s_d)\n",
    "    days = endDate-startDate\n",
    "    print(\"From {} to {} we have {} days, {} hours, and {} tweets (with {} tweets per hour)\".format(startDate, \n",
    "                                                                                                    endDate, \n",
    "                                                                                                    days.days, \n",
    "                                                                                                    days.days*24, \n",
    "                                                                                                   days.days*24*tweets_per_hour,\n",
    "                                                                                                   tweets_per_hour))\n",
    "    \n",
    "    \n",
    "countTweets('23.10.2020', '24.05.2021', 10)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2020-11-01 we have 9 days, 216 hours, and 216000 tweets (with 1000 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "countTweets('23.10.2020', '1.11.2020', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credentials validated successfully'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSearch = twitterData('/Volumes/Survey_Social_Media_Compare/Methods/')\n",
    "tSearch.validate_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitterData():\n",
    "    '''\n",
    "    A class for holding all the Twitter search related elements, from validating credentials\n",
    "    to cleaning the data.\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, main_path = '/Volumes/Survey_Social_Media_Compare/Methods/'):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.main_path = main_path\n",
    "    \n",
    "    def validate_credentials(self):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        c_path = path.join(self.main_path, 'Scripts/Twitter/twitter_keys.yaml')\n",
    "        self.credentials = load_credentials(c_path, \n",
    "                                       env_overwrite=True);\n",
    "        self.all_requests = 0;\n",
    "        self.total_results_overall = 0;\n",
    "\n",
    "        return \"Credentials validated successfully\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_query(self,\n",
    "                    mainTerms, \n",
    "                    startDate,\n",
    "                    endDate,\n",
    "                    inQuotes = True, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []):\n",
    "        \n",
    "        '''\n",
    "        Builds the query that is used to make the requests and get payloads.\n",
    "        \n",
    "        Parameters:\n",
    "            mainTerms (str): The search terms we want, e.g. 'jobs'\n",
    "            startDate (str): The lower end of the period we are interested in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 13:00'\n",
    "            endDate (str): The higher end of the period we are interested in in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 14:00'\n",
    "            inQuotes (bool): Do we want an exact phrase match? If true the terms will be put in quotes\n",
    "            language (str): Language used in the query (only languages supported by Twitter + \n",
    "                            has to be in the correct format, see https://bit.ly/2RBwmGa)\n",
    "            country (str): Country where Tweet/User is located (has to be in the correct format, see\n",
    "                            https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n",
    "            excludeRT (bool): Exclude retweets from the payload? Default False\n",
    "            results_per_call (int): How many results per request? Max is 500 for the academic API.\n",
    "            otherTerms (list): List of other search terms, e.g. ['#COVID', 'is:reply']\n",
    "        \n",
    "        Notes:\n",
    "            - More notes on building queries here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query.\n",
    "            - Tweets are fetched in reverse chronological order, i.e. starting at endDate \n",
    "            and continuing until a limit is reached.\n",
    "            - endDate refers to previous day until 23:59\n",
    "        '''\n",
    "        \n",
    "        # If excluding retweets, set rt to '-' \n",
    "        rt = '-is:retweet' if excludeRT == True else ''\n",
    "        \n",
    "        # Are the terms in quotes\n",
    "        mainTerms = '\"{}\"'.format(mainTerms) if inQuotes == True else '{}'\n",
    "        \n",
    "        # Build query text\n",
    "        queryText = '{} lang: {} place_country:{}'.format(mainTerms,\n",
    "                                                         language,\n",
    "                                                         country)\n",
    "        \n",
    "        # If there are other terms, include them in the queryText\n",
    "        queryText = queryText.extend(other) if otherTerms != [] else queryText\n",
    "        \n",
    "        # Save these as will be used to determine limits\n",
    "        self.results_per_call = results_per_call\n",
    "            \n",
    "        # Build query\n",
    "        self.query = gen_request_parameters(queryText,\n",
    "                                      start_time = startDate,\n",
    "                                      end_time = endDate,\n",
    "                                      tweet_fields = return_fields,\n",
    "                                      results_per_call = self.results_per_call)\n",
    "        \n",
    "    \n",
    "    def get_data(self, nTweets = 500):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #\n",
    "        self.rs = ResultStream(request_parameters = self.query,\n",
    "                                  max_tweets = nTweets,\n",
    "                                  output_format = \"a\",\n",
    "                                  **self.credentials)\n",
    "        \n",
    "        self.result = list(self.rs.stream())\n",
    "        \n",
    "        # We can get the total requests made for a payload using:\n",
    "        # twitterData_instance.rs.n_requests\n",
    "        # twitterData_instance.rs.session_request_counter\n",
    "        \n",
    "        # This can be used to get the overall requests made and saving logs.\n",
    "        self.all_requests += self.rs.session_request_counter       \n",
    "        self.total_results_overall += self.rs.total_results\n",
    "        \n",
    "    \n",
    "    def surveyDates(self):\n",
    "        \n",
    "        # Path to survey periods file\n",
    "        s_path = path.join(self.main_path, '/Scripts/Surveys/table_details/surveyPeriods.xlsx')\n",
    "        \n",
    "        # Load survey periods\n",
    "        self.surveyPeriods = pd.read_excel(s_path, sheet_name='AI+HPS')\n",
    "        \n",
    "        # Generate tuple of week start and end dates based on the collection dates in the Axios/Ipsos survey. \n",
    "        self.AI_weeks = [twitterData.weekFromDay(date) for date in self.surveyPeriods['A_I_start_date']]\n",
    "        \n",
    "        # Get first monday and last sunday from the A/I data collection periods\n",
    "        firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "        _, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "        # Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "        self.mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "        self.sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "        \n",
    "        # Get strings\n",
    "        self.mondays_str = mondays.strftime('%Y-%m-%d')\n",
    "        self.sundays_str = sundays.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Saving all the results in tuples\n",
    "        self.all_weeks = [(m, s) for m, s in zip(self.mondays, self.sundays)]\n",
    "        self.all_weeks_str = [(m, s) for m, s in zip(self.mondays_str, self.sundays_str)]\n",
    "    \n",
    "    \n",
    "    def createDicts(self):\n",
    "        \n",
    "        self.allData = dict.fromkeys(self.mondays_str)\n",
    "        self.logs = dict.fromkeys(self.mondays_str)\n",
    "    \n",
    "    def oneWeek(self, mainTerms, weekNum, numTweets = 1000):\n",
    "        '''\n",
    "        TODO: re-write this docstring to reflect changes. \n",
    "        Convenience function for getting all the tweets from a specified period.\n",
    "        The parameters are fed to **build_query()**, which has more parameters with the following default values:\n",
    "                    inQuotes = False, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []\n",
    "        These should either be added to the build_query() call within the current function, or the defaults changed in build_query().\n",
    "        Parameters:\n",
    "            mainTerms (str): search \n",
    "            startDate (str): week starting (format: 'YYYY-MM-DD' w, e.g. '2020-10-23')\n",
    "            endDate (str): week ending (~)\n",
    "            \n",
    "        Returns:\n",
    "            week_df (pd.DataFrame): Payload returned by the query for the specified period in df format.   \n",
    "        '''\n",
    "        \n",
    "        # Get start and end date from the week number\n",
    "        startDate = self.all_weeks_str[weekNum - 1][0]\n",
    "        endDate = self.all_weeks_str[weekNum - 1][1]\n",
    "\n",
    "        \n",
    "        # Could have also done\n",
    "        # startDate = self.mondays_str[weekNum - 1]\n",
    "        # endDate = self.sundays_str[weekNum - 1]\n",
    "        \n",
    "        # Build the query with the specified terms\n",
    "        self.build_query(mainTerms, startDate, endDate, results_per_call=500)\n",
    "        \n",
    "        # Get the data. \n",
    "        self.get_data(nTweets = numTweets) # 20,000 is quite conservative, it's unlikely we would get more than ~10,000/week.\n",
    "        \n",
    "        # Clean data (-> pd.DataFrame) and save into dictionary, with the specified by the startDate (i.e. the date corresponding to Monday of any given week in the entire period covered)\n",
    "        self.allData[startDate] = twitterData.get_df(self.result)\n",
    "        \n",
    "        # Calculate the time covered in a payload.\n",
    "        # Most recent date/time in the df in datetime format\n",
    "        self.most_recent = twitterData.toDatetime(max(self.allData[startDate]['created_at']))\n",
    "        self.oldest = twitterData.toDatetime(min(self.allData[startDate]['created_at']))\n",
    "        \n",
    "        self.timeCovered = (self.most_recent - self.oldest).seconds\n",
    "        \n",
    "        self.logs[startDate] = {\n",
    "            'mostRecent': self.most_recent,\n",
    "            'oldest': self.oldest,\n",
    "            'timeCovered': self.timeCovered,\n",
    "            'sessionRequestCounter': self.rs.session_request_counter,\n",
    "            'totalTweets': self.rs.total_results,\n",
    "            'totalTweetsOverall': self.total_results_overall,\n",
    "            'requestParams': self.rs.request_parameters\n",
    "            }\n",
    "            \n",
    "        # Save current week's Monday date.\n",
    "        # Will be used to name the files when saving.\n",
    "        self.currentWeek = startDate\n",
    "    \n",
    "    def exportOneWeek(self, saveDF = True, saveJSON = False):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if saveJSON:\n",
    "            json_path = path.join(self.main_path, '/Survey_Social_Media_Compare/Methods/Data/Twitter/JSON/{}.json'.format(self.currentWeek))\n",
    "            \n",
    "            with open(json_path, 'w') as fout:\n",
    "                json.dump(self.result, fout)\n",
    "            \n",
    "        if saveDF:\n",
    "            df_path = path.join(self.main_path, '/Survey_Social_Media_Compare/Methods/Data/Twitter/CSV/{}.csv'.format(self.currentWeek))\n",
    "            self.allData[self.currentWeek].to_csv(df_path)\n",
    "            \n",
    "    @staticmethod\n",
    "    def loadOneWeek(weekStart, loadDF = True, loadJSON = False):\n",
    "        '''\n",
    "        weekStart (str): name of file to be loaded; same format as used for currentWeek, e.g. '2020-10-19'\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if(loadDF and not loadJSON):\n",
    "            df = pd.read_csv('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/CSV/{}.csv'.format(weekStart))\n",
    "            return df\n",
    "        \n",
    "        if(not loadDF and loadJSON):\n",
    "            with open('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/JSON/{}.json'.format(weekStart)) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        if(loadDF and loadJSON):\n",
    "            df = pd.read_csv('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/CSV/{}.csv'.format(weekStart))\n",
    "            \n",
    "            with open('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/JSON/{}.json'.format(weekStart)) as f:\n",
    "                result = json.load(f)\n",
    "                    \n",
    "            return df, result     \n",
    "            \n",
    "            \n",
    "#         pd.read_csv('/Volumes/Survey_Social_Media_Compare/Methods/Data/Twitter/CSV/week1.csv', index_col=0, dtype={'id': object})\n",
    "                                                              \n",
    "                                                              \n",
    "                                                                  \n",
    "        \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def toDatetime(dateStr):\n",
    "        '''\n",
    "        Take a date in the ISO format that we get from twitter \"%Y-%m-%dT%H:%M:%S.000Z\"\n",
    "        and transform to a datetime for calculations.\n",
    "\n",
    "        Parameters:\n",
    "            dateStr (str): A date string (ISO format)\n",
    "        \n",
    "        Returns:\n",
    "            dateDT (datetime): A datetime object  \n",
    "        '''\n",
    "        \n",
    "        dateDT = datetime.strptime(dateStr, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "        \n",
    "        return dateDT\n",
    "    \n",
    "    @staticmethod\n",
    "    def weekFromDay(day):\n",
    "        '''\n",
    "        Work the week starting and ending dates given any date.\n",
    "        Params:\n",
    "            day (datetime): Can be a Timestamp (pandas/numpy object) or a datetime.datetime object.\n",
    "\n",
    "        Returns: \n",
    "            weekStart (Timestamp): The date corresponding to the start (i.e. Monday) of the date specified by *day* param.\n",
    "            weekEnd (Timestamp): The date corresponding to the end (i.e. Sunday) of the date specified by *day* param.\n",
    "        '''\n",
    "\n",
    "        weekStart = day - timedelta(days=day.weekday())\n",
    "        weekEnd = weekStart + timedelta(days=6)\n",
    "\n",
    "        return weekStart.strftime('%Y-%m-%d'), weekEnd.strftime('%Y-%m-%d')\n",
    "    \n",
    "    @staticmethod\n",
    "    def combineWeeks(dataDict):\n",
    "        pass\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_df(dictLS):\n",
    "        '''\n",
    "        '''\n",
    "        # Remove the entries (i.e. dictionaries) that contain\n",
    "        # the key 'newest_id' from the payload, i.e. the result \n",
    "        # of our query (which is a list of dictionaries).        \n",
    "        clean_json_list = [x for x in dictLS if 'newest_id' not in x]        \n",
    "        \n",
    "        df = pd.json_normalize(clean_json_list)\n",
    "    \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credentials validated successfully'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods')\n",
    "search1.validate_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1319442612487479301</td>\n",
       "      <td>Exactly..they should loose their jobs.. https:...</td>\n",
       "      <td>2020-10-23T00:56:47.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319442601444020225</td>\n",
       "      <td>@MeidasTouch Is Trump going schizo on us that ...</td>\n",
       "      <td>2020-10-23T00:56:44.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319442291749158915</td>\n",
       "      <td>Thank you @connectmeetings for getting meeting...</td>\n",
       "      <td>2020-10-23T00:55:31.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1319442241052680193</td>\n",
       "      <td>@jecoreyarthur Or another option for jobs</td>\n",
       "      <td>2020-10-23T00:55:18.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1319442109917728770</td>\n",
       "      <td>“They took our jobs!!” Bro u didnt go to colle...</td>\n",
       "      <td>2020-10-23T00:54:47.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1319442103164936193</td>\n",
       "      <td>I'm horrified by this. Any health professional...</td>\n",
       "      <td>2020-10-23T00:54:46.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1319441826332409856</td>\n",
       "      <td>Part of me says “if only Hunter didn’t take th...</td>\n",
       "      <td>2020-10-23T00:53:40.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1319441463671967746</td>\n",
       "      <td>@JohnDiesattheEn Modern debates are the NFL Bl...</td>\n",
       "      <td>2020-10-23T00:52:13.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1319441090911547392</td>\n",
       "      <td>You have?\\nThat’s all I want your amazing Earl...</td>\n",
       "      <td>2020-10-23T00:50:44.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1319440777085419521</td>\n",
       "      <td>I’d say that this is a reason some jobs can’t ...</td>\n",
       "      <td>2020-10-23T00:49:29.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1319440647552684034</td>\n",
       "      <td>@WriterDf Only that it has never happened in t...</td>\n",
       "      <td>2020-10-23T00:48:59.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1319439921212514305</td>\n",
       "      <td>Interested in a #FireSafety or #PublicSafety c...</td>\n",
       "      <td>2020-10-23T00:46:05.000Z</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1319439749191520256</td>\n",
       "      <td>@ADub1581 @WillBrinson I legitimately think Be...</td>\n",
       "      <td>2020-10-23T00:45:24.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1319439533189070848</td>\n",
       "      <td>working in retail I feel could be one of the m...</td>\n",
       "      <td>2020-10-23T00:44:33.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1319438857390247937</td>\n",
       "      <td>@AirAssets @BShipspotting @WarshipCam @Plymout...</td>\n",
       "      <td>2020-10-23T00:41:52.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1319438828499881986</td>\n",
       "      <td>@realDonaldTrump Mr. President doing a great j...</td>\n",
       "      <td>2020-10-23T00:41:45.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1319438356330115072</td>\n",
       "      <td>@KwikWarren And that’s why you don’t see him w...</td>\n",
       "      <td>2020-10-23T00:39:52.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1319437588999012352</td>\n",
       "      <td>@TroyAikman @Buck the two of you need to shut ...</td>\n",
       "      <td>2020-10-23T00:36:49.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1319437496342642688</td>\n",
       "      <td>Also, consider though that people were making ...</td>\n",
       "      <td>2020-10-23T00:36:27.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1319437375202684928</td>\n",
       "      <td>@WhiteHouse He’s the first person to kill 222,...</td>\n",
       "      <td>2020-10-23T00:35:58.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1319437260236861446</td>\n",
       "      <td>2020 in a nutshell: we have more faith in the ...</td>\n",
       "      <td>2020-10-23T00:35:31.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1319436072351301633</td>\n",
       "      <td>@jd1974815 @wendyp4545 @ChuckCallesto @tickels...</td>\n",
       "      <td>2020-10-23T00:30:48.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1319435924971859969</td>\n",
       "      <td>#VoteBlue Democrats have way more good paying ...</td>\n",
       "      <td>2020-10-23T00:30:13.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1319435615805427712</td>\n",
       "      <td>No one cares about Hunter Biden  !!! Get real ...</td>\n",
       "      <td>2020-10-23T00:28:59.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1319435474671292416</td>\n",
       "      <td>Honestly I don’t understand this. Mass and I m...</td>\n",
       "      <td>2020-10-23T00:28:25.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1319435390265208834</td>\n",
       "      <td>@MeghanMcCain He’s still a traitor! He’s made ...</td>\n",
       "      <td>2020-10-23T00:28:05.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1319434320248012804</td>\n",
       "      <td>Instead of talking about hoax's and ramming ju...</td>\n",
       "      <td>2020-10-23T00:23:50.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1319433699667050497</td>\n",
       "      <td>Put my 2 weeks in at one of my jobs today. I’m...</td>\n",
       "      <td>2020-10-23T00:21:22.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1319432261801021440</td>\n",
       "      <td>Firefighting resources are stretched thin. @ri...</td>\n",
       "      <td>2020-10-23T00:15:39.000Z</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1319432051867738113</td>\n",
       "      <td>Worked 55 hours this week from both jobs. I am...</td>\n",
       "      <td>2020-10-23T00:14:49.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1319431939565129728</td>\n",
       "      <td>@reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...</td>\n",
       "      <td>2020-10-23T00:14:22.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1319431911580774400</td>\n",
       "      <td>Nancy is a disgrace how she dare to put raises...</td>\n",
       "      <td>2020-10-23T00:14:16.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1319431528603127808</td>\n",
       "      <td>So uhm why can’t jobs just be like ‘yea so we ...</td>\n",
       "      <td>2020-10-23T00:12:44.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1319430987428761600</td>\n",
       "      <td>No person on this planet would give Trump a nu...</td>\n",
       "      <td>2020-10-23T00:10:35.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1319430894134976519</td>\n",
       "      <td>Admitting that a deadly pandemic exists, that ...</td>\n",
       "      <td>2020-10-23T00:10:13.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1319429909429760001</td>\n",
       "      <td>@yasminv @kwelkernbc Triple threats, beautiful...</td>\n",
       "      <td>2020-10-23T00:06:18.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1319429724117061633</td>\n",
       "      <td>This is the worst PR for iPhone since Steve Jo...</td>\n",
       "      <td>2020-10-23T00:05:34.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1319429167167860736</td>\n",
       "      <td>You know we have a healthy democracy when sayi...</td>\n",
       "      <td>2020-10-23T00:03:21.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1319429110033076224</td>\n",
       "      <td>@agamemnus_dev @FirenzeMike @realDonaldTrump 1...</td>\n",
       "      <td>2020-10-23T00:03:08.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1319428993288863744</td>\n",
       "      <td>Join the CVS Health team! See our latest job o...</td>\n",
       "      <td>2020-10-23T00:02:40.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0   1319442612487479301  Exactly..they should loose their jobs.. https:...   \n",
       "1   1319442601444020225  @MeidasTouch Is Trump going schizo on us that ...   \n",
       "2   1319442291749158915  Thank you @connectmeetings for getting meeting...   \n",
       "3   1319442241052680193          @jecoreyarthur Or another option for jobs   \n",
       "4   1319442109917728770  “They took our jobs!!” Bro u didnt go to colle...   \n",
       "5   1319442103164936193  I'm horrified by this. Any health professional...   \n",
       "6   1319441826332409856  Part of me says “if only Hunter didn’t take th...   \n",
       "7   1319441463671967746  @JohnDiesattheEn Modern debates are the NFL Bl...   \n",
       "8   1319441090911547392  You have?\\nThat’s all I want your amazing Earl...   \n",
       "9   1319440777085419521  I’d say that this is a reason some jobs can’t ...   \n",
       "10  1319440647552684034  @WriterDf Only that it has never happened in t...   \n",
       "11  1319439921212514305  Interested in a #FireSafety or #PublicSafety c...   \n",
       "12  1319439749191520256  @ADub1581 @WillBrinson I legitimately think Be...   \n",
       "13  1319439533189070848  working in retail I feel could be one of the m...   \n",
       "14  1319438857390247937  @AirAssets @BShipspotting @WarshipCam @Plymout...   \n",
       "15  1319438828499881986  @realDonaldTrump Mr. President doing a great j...   \n",
       "16  1319438356330115072  @KwikWarren And that’s why you don’t see him w...   \n",
       "17  1319437588999012352  @TroyAikman @Buck the two of you need to shut ...   \n",
       "18  1319437496342642688  Also, consider though that people were making ...   \n",
       "19  1319437375202684928  @WhiteHouse He’s the first person to kill 222,...   \n",
       "20  1319437260236861446  2020 in a nutshell: we have more faith in the ...   \n",
       "21  1319436072351301633  @jd1974815 @wendyp4545 @ChuckCallesto @tickels...   \n",
       "22  1319435924971859969  #VoteBlue Democrats have way more good paying ...   \n",
       "23  1319435615805427712  No one cares about Hunter Biden  !!! Get real ...   \n",
       "24  1319435474671292416  Honestly I don’t understand this. Mass and I m...   \n",
       "25  1319435390265208834  @MeghanMcCain He’s still a traitor! He’s made ...   \n",
       "26  1319434320248012804  Instead of talking about hoax's and ramming ju...   \n",
       "27  1319433699667050497  Put my 2 weeks in at one of my jobs today. I’m...   \n",
       "28  1319432261801021440  Firefighting resources are stretched thin. @ri...   \n",
       "29  1319432051867738113  Worked 55 hours this week from both jobs. I am...   \n",
       "30  1319431939565129728  @reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...   \n",
       "31  1319431911580774400  Nancy is a disgrace how she dare to put raises...   \n",
       "32  1319431528603127808  So uhm why can’t jobs just be like ‘yea so we ...   \n",
       "33  1319430987428761600  No person on this planet would give Trump a nu...   \n",
       "34  1319430894134976519  Admitting that a deadly pandemic exists, that ...   \n",
       "35  1319429909429760001  @yasminv @kwelkernbc Triple threats, beautiful...   \n",
       "36  1319429724117061633  This is the worst PR for iPhone since Steve Jo...   \n",
       "37  1319429167167860736  You know we have a healthy democracy when sayi...   \n",
       "38  1319429110033076224  @agamemnus_dev @FirenzeMike @realDonaldTrump 1...   \n",
       "39  1319428993288863744  Join the CVS Health team! See our latest job o...   \n",
       "\n",
       "                  created_at  public_metrics.retweet_count  \\\n",
       "0   2020-10-23T00:56:47.000Z                             0   \n",
       "1   2020-10-23T00:56:44.000Z                             1   \n",
       "2   2020-10-23T00:55:31.000Z                             1   \n",
       "3   2020-10-23T00:55:18.000Z                             0   \n",
       "4   2020-10-23T00:54:47.000Z                             0   \n",
       "5   2020-10-23T00:54:46.000Z                             0   \n",
       "6   2020-10-23T00:53:40.000Z                             0   \n",
       "7   2020-10-23T00:52:13.000Z                             0   \n",
       "8   2020-10-23T00:50:44.000Z                             0   \n",
       "9   2020-10-23T00:49:29.000Z                             0   \n",
       "10  2020-10-23T00:48:59.000Z                             0   \n",
       "11  2020-10-23T00:46:05.000Z                             3   \n",
       "12  2020-10-23T00:45:24.000Z                             0   \n",
       "13  2020-10-23T00:44:33.000Z                             0   \n",
       "14  2020-10-23T00:41:52.000Z                             0   \n",
       "15  2020-10-23T00:41:45.000Z                             0   \n",
       "16  2020-10-23T00:39:52.000Z                             0   \n",
       "17  2020-10-23T00:36:49.000Z                             0   \n",
       "18  2020-10-23T00:36:27.000Z                             0   \n",
       "19  2020-10-23T00:35:58.000Z                             1   \n",
       "20  2020-10-23T00:35:31.000Z                             0   \n",
       "21  2020-10-23T00:30:48.000Z                             0   \n",
       "22  2020-10-23T00:30:13.000Z                             0   \n",
       "23  2020-10-23T00:28:59.000Z                             1   \n",
       "24  2020-10-23T00:28:25.000Z                             0   \n",
       "25  2020-10-23T00:28:05.000Z                             0   \n",
       "26  2020-10-23T00:23:50.000Z                             0   \n",
       "27  2020-10-23T00:21:22.000Z                             0   \n",
       "28  2020-10-23T00:15:39.000Z                            12   \n",
       "29  2020-10-23T00:14:49.000Z                             0   \n",
       "30  2020-10-23T00:14:22.000Z                             0   \n",
       "31  2020-10-23T00:14:16.000Z                             1   \n",
       "32  2020-10-23T00:12:44.000Z                             0   \n",
       "33  2020-10-23T00:10:35.000Z                             0   \n",
       "34  2020-10-23T00:10:13.000Z                             0   \n",
       "35  2020-10-23T00:06:18.000Z                             0   \n",
       "36  2020-10-23T00:05:34.000Z                             0   \n",
       "37  2020-10-23T00:03:21.000Z                             0   \n",
       "38  2020-10-23T00:03:08.000Z                             0   \n",
       "39  2020-10-23T00:02:40.000Z                             1   \n",
       "\n",
       "    public_metrics.reply_count  public_metrics.like_count  \\\n",
       "0                            0                          0   \n",
       "1                            1                          3   \n",
       "2                            0                          3   \n",
       "3                            0                          0   \n",
       "4                            0                          0   \n",
       "5                            0                          1   \n",
       "6                            0                          2   \n",
       "7                            0                          0   \n",
       "8                            0                          0   \n",
       "9                            1                          2   \n",
       "10                           1                          0   \n",
       "11                           1                          3   \n",
       "12                           1                         11   \n",
       "13                           0                          1   \n",
       "14                           1                          1   \n",
       "15                           0                          0   \n",
       "16                           1                          2   \n",
       "17                           0                          0   \n",
       "18                           2                          1   \n",
       "19                           0                          2   \n",
       "20                           0                          0   \n",
       "21                           0                          0   \n",
       "22                           0                          0   \n",
       "23                           0                          0   \n",
       "24                           0                          0   \n",
       "25                           0                          0   \n",
       "26                           2                          4   \n",
       "27                           1                          1   \n",
       "28                           2                         18   \n",
       "29                           0                          0   \n",
       "30                           0                          1   \n",
       "31                           0                          0   \n",
       "32                           0                          0   \n",
       "33                           0                          0   \n",
       "34                           0                          4   \n",
       "35                           0                          0   \n",
       "36                           0                          0   \n",
       "37                           0                          1   \n",
       "38                           0                          0   \n",
       "39                           0                          0   \n",
       "\n",
       "    public_metrics.quote_count  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            2  \n",
       "3                            0  \n",
       "4                            0  \n",
       "5                            0  \n",
       "6                            0  \n",
       "7                            0  \n",
       "8                            0  \n",
       "9                            0  \n",
       "10                           0  \n",
       "11                           0  \n",
       "12                           1  \n",
       "13                           0  \n",
       "14                           0  \n",
       "15                           0  \n",
       "16                           0  \n",
       "17                           0  \n",
       "18                           0  \n",
       "19                           0  \n",
       "20                           0  \n",
       "21                           0  \n",
       "22                           0  \n",
       "23                           0  \n",
       "24                           0  \n",
       "25                           0  \n",
       "26                           0  \n",
       "27                           0  \n",
       "28                           3  \n",
       "29                           0  \n",
       "30                           0  \n",
       "31                           0  \n",
       "32                           0  \n",
       "33                           0  \n",
       "34                           0  \n",
       "35                           0  \n",
       "36                           0  \n",
       "37                           0  \n",
       "38                           0  \n",
       "39                           0  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a query.\n",
    "search1.build_query('jobs','2020-10-23 00:00', '2020-10-23 01:00', results_per_call=500)\n",
    "\n",
    "# Getting payload. \n",
    "# This is saved in self.results.\n",
    "search1.get_data(nTweets = 1000)\n",
    "\n",
    "# Clean data and save in a pd.DataFrame\n",
    "df1 = twitterData.get_df(search1.result)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of requests in a single query is saved in the instance attributed .rs.n_requests. \n",
    "* This is overwritten when a new request is made, but before that, this number (n_request) is added to the instance's .all_requests attribute. \n",
    "    * For example, below we can see that .n_requests = 1 after both the first and second payload (saved in df1), but .all_requests is 3. \n",
    "* The .all_requests attribute will be used for ensurign compliance with rate limits.\n",
    "    * This could be done directly through *searchtweets*, which has built-in tools (e.g. exponential back-off), by making a single query for the whole period (~280 days).\n",
    "    * However, since the period we are interested in covering here is quite big, this is probably not a good solutions (e.g. if something fails on request 5,000/7,000 all data is lost but all tweets already accessed will count towards the monthly rate limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "17\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-23T01:59:41.000Z</td>\n",
       "      <td>1319458440633339904</td>\n",
       "      <td>Many of the jobs lost this year will never com...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-23T01:59:40.000Z</td>\n",
       "      <td>1319458438943019009</td>\n",
       "      <td>I have a 401K and investments. I like seeing t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-23T01:58:56.000Z</td>\n",
       "      <td>1319458252460085249</td>\n",
       "      <td>You’re all sitting here while you’re still all...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-23T01:58:43.000Z</td>\n",
       "      <td>1319458198932381697</td>\n",
       "      <td>In case you didn’t know this, the stock market...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-23T01:58:15.000Z</td>\n",
       "      <td>1319458079759568896</td>\n",
       "      <td>@Jaycaleb8 Bro what are you saying, I live dow...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2020-10-23T01:05:01.000Z</td>\n",
       "      <td>1319444682720661504</td>\n",
       "      <td>Looking forward to hearing @JoeBiden talk abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2020-10-23T01:03:45.000Z</td>\n",
       "      <td>1319444365014609920</td>\n",
       "      <td>If Biden thinks Trump mishandled Covid 19 he n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2020-10-23T01:02:48.000Z</td>\n",
       "      <td>1319444125972914176</td>\n",
       "      <td>@rrt003 @MSNBC @DailyNewsSA Well no. Also how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2020-10-23T01:02:16.000Z</td>\n",
       "      <td>1319443992451248129</td>\n",
       "      <td>Gays dont not liking Trump as a person. Ooohh ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2020-10-23T01:01:32.000Z</td>\n",
       "      <td>1319443807641825280</td>\n",
       "      <td>@LindseyGrahamSC All economists say @JoeBiden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                   id  \\\n",
       "0   2020-10-23T01:59:41.000Z  1319458440633339904   \n",
       "1   2020-10-23T01:59:40.000Z  1319458438943019009   \n",
       "2   2020-10-23T01:58:56.000Z  1319458252460085249   \n",
       "3   2020-10-23T01:58:43.000Z  1319458198932381697   \n",
       "4   2020-10-23T01:58:15.000Z  1319458079759568896   \n",
       "..                       ...                  ...   \n",
       "59  2020-10-23T01:05:01.000Z  1319444682720661504   \n",
       "60  2020-10-23T01:03:45.000Z  1319444365014609920   \n",
       "61  2020-10-23T01:02:48.000Z  1319444125972914176   \n",
       "62  2020-10-23T01:02:16.000Z  1319443992451248129   \n",
       "63  2020-10-23T01:01:32.000Z  1319443807641825280   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Many of the jobs lost this year will never com...   \n",
       "1   I have a 401K and investments. I like seeing t...   \n",
       "2   You’re all sitting here while you’re still all...   \n",
       "3   In case you didn’t know this, the stock market...   \n",
       "4   @Jaycaleb8 Bro what are you saying, I live dow...   \n",
       "..                                                ...   \n",
       "59  Looking forward to hearing @JoeBiden talk abou...   \n",
       "60  If Biden thinks Trump mishandled Covid 19 he n...   \n",
       "61  @rrt003 @MSNBC @DailyNewsSA Well no. Also how ...   \n",
       "62  Gays dont not liking Trump as a person. Ooohh ...   \n",
       "63  @LindseyGrahamSC All economists say @JoeBiden ...   \n",
       "\n",
       "    public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                              0                           1   \n",
       "1                              0                           1   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           1   \n",
       "..                           ...                         ...   \n",
       "59                             0                           0   \n",
       "60                             0                           0   \n",
       "61                             0                           1   \n",
       "62                             2                           4   \n",
       "63                             0                           0   \n",
       "\n",
       "    public_metrics.like_count  public_metrics.quote_count  \n",
       "0                           2                           0  \n",
       "1                           1                           0  \n",
       "2                           0                           0  \n",
       "3                           1                           0  \n",
       "4                           0                           0  \n",
       "..                        ...                         ...  \n",
       "59                          2                           0  \n",
       "60                          0                           0  \n",
       "61                          1                           0  \n",
       "62                         40                           1  \n",
       "63                          0                           0  \n",
       "\n",
       "[64 rows x 7 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1.build_query('jobs','2020-10-23 01:00', '2020-10-23 2:00', results_per_call=500)\n",
    "search1.get_data(nTweets = 1000)\n",
    "df2 = twitterData.get_df(search1.result)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "18\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above also gives an indication of how many tweets to expect for our most basic query on 'jobs'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 week based on the data collection periods in the surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey periods\n",
    "# These hold the dates on which data was collected for each survey in part\n",
    "# Will use it to get twitter data from the same period.\n",
    "surveyPeriods = pd.read_excel('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Surveys/table_details/surveyPeriods.xlsx', sheet_name='AI+HPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week start(Monday): 2020-10-19 \n",
      "Week end (Sunday): 2020-10-25\n",
      "Var type: <class 'str'>,<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "p1_start, p2_start = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "print(\"Week start(Monday): {} \\nWeek end (Sunday): {}\\nVar type: {},{}\".format(p1_start, p2_start, type(p1_start), type(p2_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The need to pick up the pace on transitioning ...</td>\n",
       "      <td>1320151587403288579</td>\n",
       "      <td>2020-10-24T23:54:00.000Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We're hiring! Click to apply: Restaurant Manag...</td>\n",
       "      <td>1320151419857498112</td>\n",
       "      <td>2020-10-24T23:53:20.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‘Even on a rainy day — you can tell the Sunshi...</td>\n",
       "      <td>1320151277926555660</td>\n",
       "      <td>2020-10-24T23:52:46.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life is a struggle especially in between jobs ...</td>\n",
       "      <td>1320151038448390144</td>\n",
       "      <td>2020-10-24T23:51:49.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>💯 New York remains desirable for all the same ...</td>\n",
       "      <td>1320150725825941504</td>\n",
       "      <td>2020-10-24T23:50:34.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>@realDonaldTrump You are President we have los...</td>\n",
       "      <td>1317981967376207873</td>\n",
       "      <td>2020-10-19T00:12:42.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>@AbjornSell @StacyLStiles @JoeBiden Obama gove...</td>\n",
       "      <td>1317981499208142849</td>\n",
       "      <td>2020-10-19T00:10:50.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>@KellyO How desperate are \"reporters\" to save ...</td>\n",
       "      <td>1317980815842750466</td>\n",
       "      <td>2020-10-19T00:08:08.000Z</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>So many things wrong with this 4 min clip. \\n\\...</td>\n",
       "      <td>1317979592125276162</td>\n",
       "      <td>2020-10-19T00:03:16.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4934</th>\n",
       "      <td>Deregulation. He has deregulated polluters and...</td>\n",
       "      <td>1317979297886269441</td>\n",
       "      <td>2020-10-19T00:02:06.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4935 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                   id  \\\n",
       "0     The need to pick up the pace on transitioning ...  1320151587403288579   \n",
       "1     We're hiring! Click to apply: Restaurant Manag...  1320151419857498112   \n",
       "2     ‘Even on a rainy day — you can tell the Sunshi...  1320151277926555660   \n",
       "3     Life is a struggle especially in between jobs ...  1320151038448390144   \n",
       "4     💯 New York remains desirable for all the same ...  1320150725825941504   \n",
       "...                                                 ...                  ...   \n",
       "4930  @realDonaldTrump You are President we have los...  1317981967376207873   \n",
       "4931  @AbjornSell @StacyLStiles @JoeBiden Obama gove...  1317981499208142849   \n",
       "4932  @KellyO How desperate are \"reporters\" to save ...  1317980815842750466   \n",
       "4933  So many things wrong with this 4 min clip. \\n\\...  1317979592125276162   \n",
       "4934  Deregulation. He has deregulated polluters and...  1317979297886269441   \n",
       "\n",
       "                    created_at  public_metrics.retweet_count  \\\n",
       "0     2020-10-24T23:54:00.000Z                             8   \n",
       "1     2020-10-24T23:53:20.000Z                             0   \n",
       "2     2020-10-24T23:52:46.000Z                             2   \n",
       "3     2020-10-24T23:51:49.000Z                             0   \n",
       "4     2020-10-24T23:50:34.000Z                             0   \n",
       "...                        ...                           ...   \n",
       "4930  2020-10-19T00:12:42.000Z                             0   \n",
       "4931  2020-10-19T00:10:50.000Z                             0   \n",
       "4932  2020-10-19T00:08:08.000Z                             8   \n",
       "4933  2020-10-19T00:03:16.000Z                             0   \n",
       "4934  2020-10-19T00:02:06.000Z                             0   \n",
       "\n",
       "      public_metrics.reply_count  public_metrics.like_count  \\\n",
       "0                              0                         34   \n",
       "1                              0                          0   \n",
       "2                              1                          7   \n",
       "3                              0                          0   \n",
       "4                              0                          4   \n",
       "...                          ...                        ...   \n",
       "4930                           0                          0   \n",
       "4931                           0                          0   \n",
       "4932                          11                         43   \n",
       "4933                           0                          0   \n",
       "4934                           1                          0   \n",
       "\n",
       "      public_metrics.quote_count  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "...                          ...  \n",
       "4930                           0  \n",
       "4931                           0  \n",
       "4932                           1  \n",
       "4933                           0  \n",
       "4934                           0  \n",
       "\n",
       "[4935 rows x 7 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods')\n",
    "search2.validate_credentials()\n",
    "search2.build_query('jobs', p1_start, p2_start)\n",
    "search2.get_data(nTweets=20000) # Covering a whole week now -> higher nTweets set as the limit.\n",
    "df3 = twitterData.get_df(search2.result)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the weeks from survey data collection periods.\n",
    "\n",
    "This will be moved to the surveyDates method of twitterData. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single week start and end date as tuple. \n",
    "bla = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "\n",
    "# Get all the week's start and end date as a list of tuples.\n",
    "bla2 = [twitterData.weekFromDay(date) for date in surveyPeriods['A_I_start_date']]\n",
    "\n",
    "# Get the Monday dates.\n",
    "bla3 = [bla2[i][0] for i in range(len(bla2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This will be used later on for getting the overlap periods. \n",
    "* Howver, what we actuall want here is all the the week start and end dates from the first A/I collection period to the last (with no breaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first monday and last sunday from the A/I data collection periods\n",
    "firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "_, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "# Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "\n",
    "# Saving results in a tuple\n",
    "bla4 = (mondays[0].strftime('%Y-%m-%d'), sundays[0].strftime('%Y-%m-%d'))\n",
    "\n",
    "# Saving all the results in a tuple\n",
    "bla5 = [(m.strftime('%Y-%m-%d'), s.strftime('%Y-%m-%d')) for m, s in zip(mondays, sundays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22'),\n",
       " ('2020-11-30', '2020-12-06'),\n",
       " ('2020-12-07', '2020-12-13'),\n",
       " ('2020-12-14', '2020-12-20'),\n",
       " ('2021-01-04', '2021-01-10'),\n",
       " ('2021-01-18', '2021-01-24'),\n",
       " ('2021-01-25', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-07'),\n",
       " ('2021-02-15', '2021-02-21'),\n",
       " ('2021-02-22', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-07'),\n",
       " ('2021-03-15', '2021-03-21'),\n",
       " ('2021-03-29', '2021-04-04'),\n",
       " ('2021-04-12', '2021-04-18'),\n",
       " ('2021-05-03', '2021-05-09'),\n",
       " ('2021-05-17', '2021-05-23')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-10-26', '2020-11-01'),\n",
       " ('2020-11-02', '2020-11-08'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22'),\n",
       " ('2020-11-23', '2020-11-29'),\n",
       " ('2020-11-30', '2020-12-06'),\n",
       " ('2020-12-07', '2020-12-13'),\n",
       " ('2020-12-14', '2020-12-20'),\n",
       " ('2020-12-21', '2020-12-27'),\n",
       " ('2020-12-28', '2021-01-03'),\n",
       " ('2021-01-04', '2021-01-10'),\n",
       " ('2021-01-11', '2021-01-17'),\n",
       " ('2021-01-18', '2021-01-24'),\n",
       " ('2021-01-25', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-07'),\n",
       " ('2021-02-08', '2021-02-14'),\n",
       " ('2021-02-15', '2021-02-21'),\n",
       " ('2021-02-22', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-07'),\n",
       " ('2021-03-08', '2021-03-14'),\n",
       " ('2021-03-15', '2021-03-21'),\n",
       " ('2021-03-22', '2021-03-28'),\n",
       " ('2021-03-29', '2021-04-04'),\n",
       " ('2021-04-05', '2021-04-11'),\n",
       " ('2021-04-12', '2021-04-18'),\n",
       " ('2021-04-19', '2021-04-25'),\n",
       " ('2021-04-26', '2021-05-02'),\n",
       " ('2021-05-03', '2021-05-09'),\n",
       " ('2021-05-10', '2021-05-16'),\n",
       " ('2021-05-17', '2021-05-23')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version of the above.\n",
    "\n",
    "# Get first monday and last sunday from the A/I data collection periods\n",
    "firstDate,_ = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "_, lastDate = twitterData.weekFromDay(surveyPeriods['A_I_start_date'].iloc[-1])\n",
    "\n",
    "# Create data ranges for all mondays/sundays starting with the first one covered in A/I.\n",
    "mondays = pd.date_range(firstDate, lastDate, freq='W-MON')\n",
    "sundays = pd.date_range(firstDate, lastDate, freq='W-SUN')\n",
    "\n",
    "# Get strings\n",
    "mondays_str = mondays.strftime('%Y-%m-%d')\n",
    "sundays_str = sundays.strftime('%Y-%m-%d')\n",
    "\n",
    "# Saving all the results in a tuple\n",
    "all_weeks = [(m, s) for m, s in zip(mondays, sundays)]\n",
    "all_weeks_str = [(m, s) for m, s in zip(mondays_str, sundays_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2020-10-19 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-10-25 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-10-26 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-01 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-02 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-08 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-09 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-15 00:00:00', freq='W-SUN')),\n",
       " (Timestamp('2020-11-16 00:00:00', freq='W-MON'),\n",
       "  Timestamp('2020-11-22 00:00:00', freq='W-SUN'))]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-10-19', '2020-10-25'),\n",
       " ('2020-10-26', '2020-11-01'),\n",
       " ('2020-11-02', '2020-11-08'),\n",
       " ('2020-11-09', '2020-11-15'),\n",
       " ('2020-11-16', '2020-11-22')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks_str[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020-10-19', '2020-10-26', '2020-11-02', '2020-11-09', '2020-11-16',\n",
       "       '2020-11-23', '2020-11-30', '2020-12-07', '2020-12-14', '2020-12-21',\n",
       "       '2020-12-28', '2021-01-04', '2021-01-11', '2021-01-18', '2021-01-25',\n",
       "       '2021-02-01', '2021-02-08', '2021-02-15', '2021-02-22', '2021-03-01',\n",
       "       '2021-03-08', '2021-03-15', '2021-03-22', '2021-03-29', '2021-04-05',\n",
       "       '2021-04-12', '2021-04-19', '2021-04-26', '2021-05-03', '2021-05-10',\n",
       "       '2021-05-17'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondays_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitterSearch",
   "language": "python",
   "name": "twittersearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
