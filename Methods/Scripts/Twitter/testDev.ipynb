{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "### Things that need to be done    \n",
    "\n",
    "- [x] Set up skeleton for a nice object oriented approach. \n",
    "- [x] Figure out best way to get tweets.\n",
    "- [x] Data format\n",
    "- [x] Build class and funcs\n",
    "- [x] Figure out how to best count all requests in a session and make sure it's functional\n",
    "    - This is already built-in to searchtweets to some extent, so will use that.\n",
    "- [x] Make oneDay() fail safe (it's a bad idea to get the data and combine it within a single for, because if something goes wrong in an interation all the data from previous iterations will be lost if something goes wrong).\n",
    "    - Getting a whole week instead\n",
    "- [ ] Change oneDay() to oneWeek().    \n",
    "- [ ] Save metadata for the payloads (i.e. self.most_recent, self.oldest, self.timeCovered, rs.total_results + other?)\n",
    "    - Seach tweets already saves some sort of log, print this to file with the appropriate name.  \n",
    "\n",
    "### Rate Limits\n",
    "\n",
    "- 10,000,000 Tweets per month (resets on the 19th of each month). \n",
    "- 300 requests/15 minute window, with 500 Tweets/request:\n",
    "    - 150,000 tweets/15min \n",
    "    - 600,000 tweets/hour\n",
    "\n",
    "### How many tweets to get?\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - ~ 280 days\n",
    "    - ~ 6720 hours\n",
    "    - If we get 1000 tweets per hour: $6,720,000 * 2$. \n",
    "    - That's very little in terms of space, but might take quite a while for it to go through sentiment analysis.\n",
    "    - It would take ~22 hours to get the whole data (due to rate limits).\n",
    "    - However, it's unlikely that our queries would return anywhere near 1,000 results/hour.\n",
    "\n",
    "### Best way to get tweets\n",
    "- Period covered is: Oct 23rd - July 30th (-ish)\n",
    "    - $n_h$ per hour/day\n",
    "    - $n_d$ per day (where $n_d$ would be ~ $n_h*24$)\n",
    "    - $n_w$ per week (where $n_w$ would be ~ $n_h*24 *7$)  \n",
    "    \n",
    "\n",
    "- $n_w$ is probably the best options: \n",
    "    - can leverage functions built into *searchtweets* to avoid rate limit violations (e.g. exponential back-off).\n",
    "    - it's easy to select tweets in any given day/hour from these data.\n",
    "\n",
    "### Data format\n",
    "\n",
    "- A single results call: **JSON to pd**.\n",
    "    - This is relatively straightforward with one minor complication, i.e. entries such as this:\n",
    "    <blockquote>{'newest_id': '1402310241992183808',\n",
    "  'oldest_id': '1402310139630211083',\n",
    "  'result_count': 100,\n",
    "  'next_token': 'b26v89c19zqg8o3fpdg7rbcqdq8stpgmibslekg3kxail'}\n",
    "    </blockquote>\n",
    "    - This is used by the wrapper to get the next lot of tweets if max_tweets > results_per_call, but will also always be the last entry in a result.\n",
    "    \n",
    "    \n",
    "- Multiple result calls: **pds in dict/dict-of-dict**. \n",
    "    - I am thinking the best way to store all the data would be a dict of dataframes, but will see how it works  \n",
    "    \n",
    "### Survey periods\n",
    "\n",
    "| Period | A_I_start_date | A_I_end_date | A_I_week | HPS_start_date | HPS_end_date | HPS_Week | HPS Topic |\n",
    "|--------|----------------|--------------|----------|----------------|--------------|----------|-----------|\n",
    "| P1     | 23.10.2020     | 26.10.2020   | W29*     | 28.10.2020     | 09.11.2020   | W18      |     E    |\n",
    "| P2     | 13.11.2020     | 16.11.2020   | W30      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P2     | 20.11.2020     | 23.11.2020   | W31      | 11.11.2020     | 23.11.2020   | W19      |     E    |\n",
    "| P3     | 04.12.2020     | 07.12.2020   | W32      | 25.11.2020     | 07.12.2020   | W20      |     E    |\n",
    "| P4     | 11.12.2020     | 14.12.2020   | W33      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P4     | 18.12.2020     | 21.12.2020   | W34      | 09.12.2020     | 21.12.2020   | W21      |     E    |\n",
    "| P5     | 08.01.2021     | 11.01.2021   | W35      | 06.01.2021     | 18.01.2021   | W22      |    E,V   |\n",
    "| P6     | 22.01.2021     | 25.01.2021   | W36      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P6     | 29.01.2021     | 01.02.2021   | W37      | 20.01.2021     | 01.02.2021   | W23      |    E,V   |\n",
    "| P7     | 05.02.2021     | 08.02.2021   | W38      | 03.02.2021     | 15.02.2021   | W24      |    E,V   |\n",
    "| P8     | 19.02.2021     | 22.02.2021   | W39      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P8     | 28.02.2021     | 01.03.2021   | W40      | 17.02.2021     | 01.03.2021   | W25      |    E,V   |\n",
    "| P9     | 05.03.2021     | 08.03.2021   | W41      | 03.03.2021     | 15.03.2021   | W26      |    E,V   |\n",
    "| P10    | 19.03.2021     | 22.03.2021   | W42      | 17.03.2021     | 29.03.2021   | W27      |    E,V   |\n",
    "| P11    | 02.04.2021     | 05.04.2021   | W43      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P11    | 16.04.2021     | 19.04.2021   | W44      | 14.04.2021     | 26.04.2021   | W28      |    E,V   |\n",
    "| P12    | 07.05.2021     | 10.05.2021   | W45      | 28.04.2021     | 10.05.2021   | W29      |    E,V   |\n",
    "| P13    | 21.05.2021     | 24.05.2021   | W46      |                |              | W30      |    E,V   |\n",
    "\n",
    "\n",
    "e.g. P1: 23.10.20 - 26.10.20 (Fri - Mon)\n",
    "* Corresponding week 19.19.20 - 25.10.20 or 26.10.20 - 01.11.20?\n",
    "* For now let's say the former. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "from os import path\n",
    "from searchtweets import ResultStream, gen_request_parameters, load_credentials, collect_results, convert_utc_time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2021-05-24 we have 213 days, 5112 hours, and 51120 tweets (with 10 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "def countTweets(startDate, endDate, tweets_per_hour):\n",
    "    '''\n",
    "    Specify dates in DD.MM.YYY format (no leading 0 for months or days)\n",
    "    '''\n",
    "    \n",
    "    s_d, s_m, s_y = [ int(i) for i in startDate.split('.')]\n",
    "    e_d, e_m, e_y = [ int(i) for i in endDate.split('.')]\n",
    "\n",
    "    endDate = date(e_y, e_m, e_d)\n",
    "    startDate = date(s_y, s_m, s_d)\n",
    "    days = endDate-startDate\n",
    "    print(\"From {} to {} we have {} days, {} hours, and {} tweets (with {} tweets per hour)\".format(startDate, \n",
    "                                                                                                    endDate, \n",
    "                                                                                                    days.days, \n",
    "                                                                                                    days.days*24, \n",
    "                                                                                                   days.days*24*tweets_per_hour,\n",
    "                                                                                                   tweets_per_hour))\n",
    "    \n",
    "    \n",
    "countTweets('23.10.2020', '24.05.2021', 10)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2020-10-23 to 2020-11-01 we have 9 days, 216 hours, and 216000 tweets (with 1000 tweets per hour)\n"
     ]
    }
   ],
   "source": [
    "countTweets('23.10.2020', '1.11.2020', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitterData():\n",
    "    '''\n",
    "    A class for holding all the Twitter search related elements, from validating credentials\n",
    "    to cleaning the data.\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, main_path):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self.main_path = main_path\n",
    "    \n",
    "    def validate_credentials(self):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        c_path = path.join(self.main_path, 'twitter_keys.yaml')\n",
    "        self.credentials = load_credentials('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Twitter/twitter_keys.yaml', \n",
    "                                       env_overwrite=True);\n",
    "        self.all_requests = 0;\n",
    "\n",
    "        return \"Credentials validated successfully\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_query(self,\n",
    "                    mainTerms, \n",
    "                    startDate,\n",
    "                    endDate,\n",
    "                    inQuotes = True, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []):\n",
    "        \n",
    "        '''\n",
    "        Builds the query that is used to make the requests and get payloads.\n",
    "        \n",
    "        Parameters:\n",
    "            mainTerms (str): The search terms we want, e.g. 'jobs'\n",
    "            startDate (str): The lower end of the period we are interested in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 13:00'\n",
    "            endDate (str): The higher end of the period we are interested in in YYY-MM-DD HH:MM format, \n",
    "                             e.g. '2020-10-23 14:00'\n",
    "            inQuotes (bool): Do we want an exact phrase match? If true the terms will be put in quotes\n",
    "            language (str): Language used in the query (only languages supported by Twitter + \n",
    "                            has to be in the correct format, see https://bit.ly/2RBwmGa)\n",
    "            country (str): Country where Tweet/User is located (has to be in the correct format, see\n",
    "                            https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n",
    "            excludeRT (bool): Exclude retweets from the payload? Default False\n",
    "            results_per_call (int): How many results per request? Max is 500 for the academic API.\n",
    "            otherTerms (list): List of other search terms, e.g. ['#COVID', 'is:reply']\n",
    "        \n",
    "        Notes:\n",
    "            - More notes on building queries here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query.\n",
    "            - Tweets are fetched in reverse chronological order, i.e. starting at endDate \n",
    "            and continuing until a limit is reached.\n",
    "            - endDate refers to previous day until 23:59\n",
    "        '''\n",
    "        \n",
    "        # If excluding retweets, set rt to '-' \n",
    "        rt = '-is:retweet' if excludeRT == True else ''\n",
    "        \n",
    "        # Are the terms in quotes\n",
    "        mainTerms = '\"{}\"'.format(mainTerms) if inQuotes == True else '{}'\n",
    "        \n",
    "        # Build query text\n",
    "        queryText = '{} lang: {} place_country:{}'.format(mainTerms,\n",
    "                                                         language,\n",
    "                                                         country)\n",
    "        \n",
    "        # If there are other terms, include them in the queryText\n",
    "        queryText = queryText.extend(other) if otherTerms != [] else queryText\n",
    "        \n",
    "        # Save these as will be used to determine limits\n",
    "        self.results_per_call = results_per_call\n",
    "        self.startDate = startDate\n",
    "        self.endDate = endDate\n",
    "            \n",
    "        # Build query\n",
    "        self.query = gen_request_parameters(queryText,\n",
    "                                      start_time = self.startDate,\n",
    "                                      end_time = self.endDate,\n",
    "                                      tweet_fields = return_fields,\n",
    "                                      results_per_call = self.results_per_call)\n",
    "        \n",
    "    \n",
    "    def get_data(self, nTweets = 500):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #\n",
    "        self.rs = ResultStream(request_parameters = self.query,\n",
    "                                  max_tweets = nTweets,\n",
    "                                  output_format = \"a\",\n",
    "                                  **self.credentials)\n",
    "        \n",
    "        self.result = list(self.rs.stream())\n",
    "        \n",
    "        # We can get the total requests made for a payload using:\n",
    "        # twitterData_instance.rs.n_requests\n",
    "        # twitterData_instance.rs.session_request_counter\n",
    "        \n",
    "        # This can be used to get the overall requests made\n",
    "        self.all_requests += self.rs.session_request_counter       \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_df(self):\n",
    "        '''\n",
    "        '''\n",
    "        # Remove the entries (i.e. dictionaries) that contain\n",
    "        # the key 'newest_id' from the payload, i.e. the result \n",
    "        # of our query (which is a list of dictionaries).        \n",
    "        clean_json_list = [x for x in self.result if 'newest_id' not in x]        \n",
    "        \n",
    "        df = pd.json_normalize(clean_json_list)\n",
    "\n",
    "        # Calculate the time covered in a payload.\n",
    "        # Most recent date/time in the df in datetime format\n",
    "        self.most_recent = twitterData.toDatetime(max(df['created_at']))\n",
    "        self.oldest = twitterData.toDatetime(min(df['created_at']))\n",
    "        \n",
    "        self.timeCovered = (self.most_recent - self.oldest).seconds\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def oneWeek(self, mainTerms, startDate, endDate):\n",
    "        '''\n",
    "        Convenience function for getting all the tweets from a specified period.\n",
    "        The parameters are fed to **build_query()**, which has more parameters with the following default values:\n",
    "                    inQuotes = False, \n",
    "                    language = 'en', \n",
    "                    country = 'US',\n",
    "                    excludeRT = False,\n",
    "                    results_per_call = 500,\n",
    "                    return_fields = 'id,created_at,text,public_metrics',\n",
    "                    otherTerms = []\n",
    "        These should either be added to the build_query() call within the current function, or the defaults changed in build_query().\n",
    "        Parameters:\n",
    "            mainTerms (str): search \n",
    "            startDate (str): week starting (format: 'YYYY-MM-DD' w, e.g. '2020-10-23')\n",
    "            endDate (str): week ending (~)\n",
    "            \n",
    "        Returns:\n",
    "            week_df (pd.DataFrame): Payload returned by the query for the specified period in df format.   \n",
    "        '''\n",
    "        \n",
    "        self.build_query(mainTerms, startDate, endDate, results_per_call=500)\n",
    "        self.get_data(nTweets = 1000)\n",
    "        \n",
    "        df1 = self.get_df()\n",
    "    \n",
    "        \n",
    "#     def oneDay(self,dateStart, dateEnd):\n",
    "#         '''\n",
    "#         Get the df for a single week (specified by dateStart and dateEnd).\n",
    "#         '''\n",
    "        \n",
    "#         init_request_session = time.time()\n",
    "        \n",
    "#         # Hours in the day\n",
    "#         t = ['{}:00'.format(x) for x in range(0,24)]\n",
    "        \n",
    "#         # For every hour (of 24)\n",
    "#         for i in range(24):\n",
    "            \n",
    "#             # Determine start and end time, e.g. '2020-10-23 00:00' abd '2020-10-23 01:00' \n",
    "#             startTime = '{} {}'.format(date, t[i]) \n",
    "            \n",
    "#             if i==23:\n",
    "#                 endDate = '{} {}'.format(date, '23:59') \n",
    "#             else:\n",
    "#                 endDate = '{} {}'.format(date, t[i+1]) \n",
    "\n",
    "#             # Build the query\n",
    "#             self.build_query('jobs', startDate, endDate, results_per_call=500)\n",
    "            \n",
    "            \n",
    "#             # If the next request is the 300th (or multiple thereof)\n",
    "#             # and we are within the same 15 min window.\n",
    "#             # TODO: This is no good, as all_requests is not incremented by 1 (but by 3-30ish on every call)\n",
    "#             if (self.all_requests+1 % 300 != 0) and (time.time() - init_request_session < 900):\n",
    "                \n",
    "#                 # Get the data (up to 1000 results per hour)\n",
    "#                 self.get_data(nTweets = 1000)\n",
    "                \n",
    "#             else:\n",
    "#                 # Sleep for 15 minutes minus however long we had in this session\n",
    "#                 time.sleep(900 - (time.time() - init_request_session))\n",
    "                \n",
    "#                 # Then get the data\n",
    "#                 self.get_data(nTweets = 1000)\n",
    "                \n",
    "#                 # And reset the session timer\n",
    "#                 init_request_session = time.time()\n",
    "                \n",
    "            \n",
    "#             # Clean data\n",
    "#             current_df = self.get_df()\n",
    "            \n",
    "#             # Add to dataframe containing the data for a single day.\n",
    "#             if i == 1:\n",
    "#                 all_day_df = current_df\n",
    "                \n",
    "#             else:\n",
    "#                 all_day_df = pd.concat([all_day_df, current_df])\n",
    "\n",
    "        \n",
    "    \n",
    "    def exportOneDay():\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def exportOneDay():\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def toDatetime(dateStr):\n",
    "        '''\n",
    "        Take a date in the ISO format that we get from twitter \"%Y-%m-%dT%H:%M:%S.000Z\"\n",
    "        and transform to a datetime for calculations.\n",
    "\n",
    "        Parameters:\n",
    "            dateStr (str): A date string (ISO format)\n",
    "        \n",
    "        Returns:\n",
    "            dateDT (datetime): A datetime object  \n",
    "        '''\n",
    "        \n",
    "        dateDT = datetime.strptime(dateStr, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "        \n",
    "        return dateDT\n",
    "    \n",
    "    @staticmethod\n",
    "    def weekFromDay(day):\n",
    "        '''\n",
    "        Work the week starting and ending dates given any date.\n",
    "        Params:\n",
    "            day (datetime): Can be a Timestamp (pandas/numpy object) or a datetime.datetime object.\n",
    "\n",
    "        Returns: \n",
    "            weekStart (Timestamp): The date corresponding to the start (i.e. Monday) of the date specified by *day* param.\n",
    "            weekEnd (Timestamp): The date corresponding to the end (i.e. Sunday) of the date specified by *day* param.\n",
    "        '''\n",
    "\n",
    "        weekStart = day - timedelta(days=day.weekday())\n",
    "        weekEnd = weekStart + timedelta(days=6)\n",
    "\n",
    "        return weekStart.strftime('%Y-%m-%d'), weekEnd.strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credentials validated successfully'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Twitter/')\n",
    "search1.validate_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1319442612487479301</td>\n",
       "      <td>2020-10-23T00:56:47.000Z</td>\n",
       "      <td>Exactly..they should loose their jobs.. https:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319442601444020225</td>\n",
       "      <td>2020-10-23T00:56:44.000Z</td>\n",
       "      <td>@MeidasTouch Is Trump going schizo on us that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319442291749158915</td>\n",
       "      <td>2020-10-23T00:55:31.000Z</td>\n",
       "      <td>Thank you @connectmeetings for getting meeting...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1319442241052680193</td>\n",
       "      <td>2020-10-23T00:55:18.000Z</td>\n",
       "      <td>@jecoreyarthur Or another option for jobs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1319442109917728770</td>\n",
       "      <td>2020-10-23T00:54:47.000Z</td>\n",
       "      <td>“They took our jobs!!” Bro u didnt go to colle...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1319442103164936193</td>\n",
       "      <td>2020-10-23T00:54:46.000Z</td>\n",
       "      <td>I'm horrified by this. Any health professional...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1319441826332409856</td>\n",
       "      <td>2020-10-23T00:53:40.000Z</td>\n",
       "      <td>Part of me says “if only Hunter didn’t take th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1319441463671967746</td>\n",
       "      <td>2020-10-23T00:52:13.000Z</td>\n",
       "      <td>@JohnDiesattheEn Modern debates are the NFL Bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1319441090911547392</td>\n",
       "      <td>2020-10-23T00:50:44.000Z</td>\n",
       "      <td>You have?\\nThat’s all I want your amazing Earl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1319440777085419521</td>\n",
       "      <td>2020-10-23T00:49:29.000Z</td>\n",
       "      <td>I’d say that this is a reason some jobs can’t ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1319440647552684034</td>\n",
       "      <td>2020-10-23T00:48:59.000Z</td>\n",
       "      <td>@WriterDf Only that it has never happened in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1319439921212514305</td>\n",
       "      <td>2020-10-23T00:46:05.000Z</td>\n",
       "      <td>Interested in a #FireSafety or #PublicSafety c...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1319439749191520256</td>\n",
       "      <td>2020-10-23T00:45:24.000Z</td>\n",
       "      <td>@ADub1581 @WillBrinson I legitimately think Be...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1319439533189070848</td>\n",
       "      <td>2020-10-23T00:44:33.000Z</td>\n",
       "      <td>working in retail I feel could be one of the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1319438857390247937</td>\n",
       "      <td>2020-10-23T00:41:52.000Z</td>\n",
       "      <td>@AirAssets @BShipspotting @WarshipCam @Plymout...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1319438828499881986</td>\n",
       "      <td>2020-10-23T00:41:45.000Z</td>\n",
       "      <td>@realDonaldTrump Mr. President doing a great j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1319438356330115072</td>\n",
       "      <td>2020-10-23T00:39:52.000Z</td>\n",
       "      <td>@KwikWarren And that’s why you don’t see him w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1319437588999012352</td>\n",
       "      <td>2020-10-23T00:36:49.000Z</td>\n",
       "      <td>@TroyAikman @Buck the two of you need to shut ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1319437496342642688</td>\n",
       "      <td>2020-10-23T00:36:27.000Z</td>\n",
       "      <td>Also, consider though that people were making ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1319437375202684928</td>\n",
       "      <td>2020-10-23T00:35:58.000Z</td>\n",
       "      <td>@WhiteHouse He’s the first person to kill 222,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1319437260236861446</td>\n",
       "      <td>2020-10-23T00:35:31.000Z</td>\n",
       "      <td>2020 in a nutshell: we have more faith in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1319436072351301633</td>\n",
       "      <td>2020-10-23T00:30:48.000Z</td>\n",
       "      <td>@jd1974815 @wendyp4545 @ChuckCallesto @tickels...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1319435924971859969</td>\n",
       "      <td>2020-10-23T00:30:13.000Z</td>\n",
       "      <td>#VoteBlue Democrats have way more good paying ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1319435615805427712</td>\n",
       "      <td>2020-10-23T00:28:59.000Z</td>\n",
       "      <td>No one cares about Hunter Biden  !!! Get real ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1319435474671292416</td>\n",
       "      <td>2020-10-23T00:28:25.000Z</td>\n",
       "      <td>Honestly I don’t understand this. Mass and I m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1319435390265208834</td>\n",
       "      <td>2020-10-23T00:28:05.000Z</td>\n",
       "      <td>@MeghanMcCain He’s still a traitor! He’s made ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1319434320248012804</td>\n",
       "      <td>2020-10-23T00:23:50.000Z</td>\n",
       "      <td>Instead of talking about hoax's and ramming ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1319433699667050497</td>\n",
       "      <td>2020-10-23T00:21:22.000Z</td>\n",
       "      <td>Put my 2 weeks in at one of my jobs today. I’m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1319432261801021440</td>\n",
       "      <td>2020-10-23T00:15:39.000Z</td>\n",
       "      <td>Firefighting resources are stretched thin. @ri...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1319432051867738113</td>\n",
       "      <td>2020-10-23T00:14:49.000Z</td>\n",
       "      <td>Worked 55 hours this week from both jobs. I am...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1319431939565129728</td>\n",
       "      <td>2020-10-23T00:14:22.000Z</td>\n",
       "      <td>@reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1319431911580774400</td>\n",
       "      <td>2020-10-23T00:14:16.000Z</td>\n",
       "      <td>Nancy is a disgrace how she dare to put raises...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1319431528603127808</td>\n",
       "      <td>2020-10-23T00:12:44.000Z</td>\n",
       "      <td>So uhm why can’t jobs just be like ‘yea so we ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1319430987428761600</td>\n",
       "      <td>2020-10-23T00:10:35.000Z</td>\n",
       "      <td>No person on this planet would give Trump a nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1319430894134976519</td>\n",
       "      <td>2020-10-23T00:10:13.000Z</td>\n",
       "      <td>Admitting that a deadly pandemic exists, that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1319429909429760001</td>\n",
       "      <td>2020-10-23T00:06:18.000Z</td>\n",
       "      <td>@yasminv @kwelkernbc Triple threats, beautiful...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1319429724117061633</td>\n",
       "      <td>2020-10-23T00:05:34.000Z</td>\n",
       "      <td>This is the worst PR for iPhone since Steve Jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1319429167167860736</td>\n",
       "      <td>2020-10-23T00:03:21.000Z</td>\n",
       "      <td>You know we have a healthy democracy when sayi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1319429110033076224</td>\n",
       "      <td>2020-10-23T00:03:08.000Z</td>\n",
       "      <td>@agamemnus_dev @FirenzeMike @realDonaldTrump 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1319428993288863744</td>\n",
       "      <td>2020-10-23T00:02:40.000Z</td>\n",
       "      <td>Join the CVS Health team! See our latest job o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                created_at  \\\n",
       "0   1319442612487479301  2020-10-23T00:56:47.000Z   \n",
       "1   1319442601444020225  2020-10-23T00:56:44.000Z   \n",
       "2   1319442291749158915  2020-10-23T00:55:31.000Z   \n",
       "3   1319442241052680193  2020-10-23T00:55:18.000Z   \n",
       "4   1319442109917728770  2020-10-23T00:54:47.000Z   \n",
       "5   1319442103164936193  2020-10-23T00:54:46.000Z   \n",
       "6   1319441826332409856  2020-10-23T00:53:40.000Z   \n",
       "7   1319441463671967746  2020-10-23T00:52:13.000Z   \n",
       "8   1319441090911547392  2020-10-23T00:50:44.000Z   \n",
       "9   1319440777085419521  2020-10-23T00:49:29.000Z   \n",
       "10  1319440647552684034  2020-10-23T00:48:59.000Z   \n",
       "11  1319439921212514305  2020-10-23T00:46:05.000Z   \n",
       "12  1319439749191520256  2020-10-23T00:45:24.000Z   \n",
       "13  1319439533189070848  2020-10-23T00:44:33.000Z   \n",
       "14  1319438857390247937  2020-10-23T00:41:52.000Z   \n",
       "15  1319438828499881986  2020-10-23T00:41:45.000Z   \n",
       "16  1319438356330115072  2020-10-23T00:39:52.000Z   \n",
       "17  1319437588999012352  2020-10-23T00:36:49.000Z   \n",
       "18  1319437496342642688  2020-10-23T00:36:27.000Z   \n",
       "19  1319437375202684928  2020-10-23T00:35:58.000Z   \n",
       "20  1319437260236861446  2020-10-23T00:35:31.000Z   \n",
       "21  1319436072351301633  2020-10-23T00:30:48.000Z   \n",
       "22  1319435924971859969  2020-10-23T00:30:13.000Z   \n",
       "23  1319435615805427712  2020-10-23T00:28:59.000Z   \n",
       "24  1319435474671292416  2020-10-23T00:28:25.000Z   \n",
       "25  1319435390265208834  2020-10-23T00:28:05.000Z   \n",
       "26  1319434320248012804  2020-10-23T00:23:50.000Z   \n",
       "27  1319433699667050497  2020-10-23T00:21:22.000Z   \n",
       "28  1319432261801021440  2020-10-23T00:15:39.000Z   \n",
       "29  1319432051867738113  2020-10-23T00:14:49.000Z   \n",
       "30  1319431939565129728  2020-10-23T00:14:22.000Z   \n",
       "31  1319431911580774400  2020-10-23T00:14:16.000Z   \n",
       "32  1319431528603127808  2020-10-23T00:12:44.000Z   \n",
       "33  1319430987428761600  2020-10-23T00:10:35.000Z   \n",
       "34  1319430894134976519  2020-10-23T00:10:13.000Z   \n",
       "35  1319429909429760001  2020-10-23T00:06:18.000Z   \n",
       "36  1319429724117061633  2020-10-23T00:05:34.000Z   \n",
       "37  1319429167167860736  2020-10-23T00:03:21.000Z   \n",
       "38  1319429110033076224  2020-10-23T00:03:08.000Z   \n",
       "39  1319428993288863744  2020-10-23T00:02:40.000Z   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Exactly..they should loose their jobs.. https:...   \n",
       "1   @MeidasTouch Is Trump going schizo on us that ...   \n",
       "2   Thank you @connectmeetings for getting meeting...   \n",
       "3           @jecoreyarthur Or another option for jobs   \n",
       "4   “They took our jobs!!” Bro u didnt go to colle...   \n",
       "5   I'm horrified by this. Any health professional...   \n",
       "6   Part of me says “if only Hunter didn’t take th...   \n",
       "7   @JohnDiesattheEn Modern debates are the NFL Bl...   \n",
       "8   You have?\\nThat’s all I want your amazing Earl...   \n",
       "9   I’d say that this is a reason some jobs can’t ...   \n",
       "10  @WriterDf Only that it has never happened in t...   \n",
       "11  Interested in a #FireSafety or #PublicSafety c...   \n",
       "12  @ADub1581 @WillBrinson I legitimately think Be...   \n",
       "13  working in retail I feel could be one of the m...   \n",
       "14  @AirAssets @BShipspotting @WarshipCam @Plymout...   \n",
       "15  @realDonaldTrump Mr. President doing a great j...   \n",
       "16  @KwikWarren And that’s why you don’t see him w...   \n",
       "17  @TroyAikman @Buck the two of you need to shut ...   \n",
       "18  Also, consider though that people were making ...   \n",
       "19  @WhiteHouse He’s the first person to kill 222,...   \n",
       "20  2020 in a nutshell: we have more faith in the ...   \n",
       "21  @jd1974815 @wendyp4545 @ChuckCallesto @tickels...   \n",
       "22  #VoteBlue Democrats have way more good paying ...   \n",
       "23  No one cares about Hunter Biden  !!! Get real ...   \n",
       "24  Honestly I don’t understand this. Mass and I m...   \n",
       "25  @MeghanMcCain He’s still a traitor! He’s made ...   \n",
       "26  Instead of talking about hoax's and ramming ju...   \n",
       "27  Put my 2 weeks in at one of my jobs today. I’m...   \n",
       "28  Firefighting resources are stretched thin. @ri...   \n",
       "29  Worked 55 hours this week from both jobs. I am...   \n",
       "30  @reksveks @CooljoshuaXD @QQm0ar @hutchinson @W...   \n",
       "31  Nancy is a disgrace how she dare to put raises...   \n",
       "32  So uhm why can’t jobs just be like ‘yea so we ...   \n",
       "33  No person on this planet would give Trump a nu...   \n",
       "34  Admitting that a deadly pandemic exists, that ...   \n",
       "35  @yasminv @kwelkernbc Triple threats, beautiful...   \n",
       "36  This is the worst PR for iPhone since Steve Jo...   \n",
       "37  You know we have a healthy democracy when sayi...   \n",
       "38  @agamemnus_dev @FirenzeMike @realDonaldTrump 1...   \n",
       "39  Join the CVS Health team! See our latest job o...   \n",
       "\n",
       "    public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                              0                           0   \n",
       "1                              1                           1   \n",
       "2                              1                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           0   \n",
       "5                              0                           0   \n",
       "6                              0                           0   \n",
       "7                              0                           0   \n",
       "8                              0                           0   \n",
       "9                              0                           1   \n",
       "10                             0                           1   \n",
       "11                             3                           1   \n",
       "12                             0                           1   \n",
       "13                             0                           0   \n",
       "14                             0                           1   \n",
       "15                             0                           0   \n",
       "16                             0                           1   \n",
       "17                             0                           0   \n",
       "18                             0                           2   \n",
       "19                             1                           0   \n",
       "20                             0                           0   \n",
       "21                             0                           0   \n",
       "22                             0                           0   \n",
       "23                             1                           0   \n",
       "24                             0                           0   \n",
       "25                             0                           0   \n",
       "26                             0                           2   \n",
       "27                             0                           1   \n",
       "28                            12                           2   \n",
       "29                             0                           0   \n",
       "30                             0                           0   \n",
       "31                             1                           0   \n",
       "32                             0                           0   \n",
       "33                             0                           0   \n",
       "34                             0                           0   \n",
       "35                             0                           0   \n",
       "36                             0                           0   \n",
       "37                             0                           0   \n",
       "38                             0                           0   \n",
       "39                             1                           0   \n",
       "\n",
       "    public_metrics.like_count  public_metrics.quote_count  \n",
       "0                           0                           0  \n",
       "1                           3                           0  \n",
       "2                           3                           2  \n",
       "3                           0                           0  \n",
       "4                           0                           0  \n",
       "5                           1                           0  \n",
       "6                           2                           0  \n",
       "7                           0                           0  \n",
       "8                           0                           0  \n",
       "9                           2                           0  \n",
       "10                          0                           0  \n",
       "11                          3                           0  \n",
       "12                         11                           1  \n",
       "13                          1                           0  \n",
       "14                          1                           0  \n",
       "15                          0                           0  \n",
       "16                          2                           0  \n",
       "17                          0                           0  \n",
       "18                          1                           0  \n",
       "19                          2                           0  \n",
       "20                          0                           0  \n",
       "21                          0                           0  \n",
       "22                          0                           0  \n",
       "23                          0                           0  \n",
       "24                          0                           0  \n",
       "25                          0                           0  \n",
       "26                          4                           0  \n",
       "27                          1                           0  \n",
       "28                         18                           3  \n",
       "29                          0                           0  \n",
       "30                          1                           0  \n",
       "31                          0                           0  \n",
       "32                          0                           0  \n",
       "33                          0                           0  \n",
       "34                          4                           0  \n",
       "35                          0                           0  \n",
       "36                          0                           0  \n",
       "37                          1                           0  \n",
       "38                          0                           0  \n",
       "39                          0                           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a query.\n",
    "search1.build_query('jobs','2020-10-23 00:00', '2020-10-23 01:00', results_per_call=500)\n",
    "\n",
    "# Getting payload. \n",
    "# This is saved in self.results.\n",
    "search1.get_data(nTweets = 1000)\n",
    "\n",
    "# Clean data and save in a pd.DataFrame\n",
    "df1 = search1.get_df()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of requests in a single query is saved in the instance attributed .rs.n_requests. \n",
    "* This is overwritten when a new request is made, but before that, this number (n_request) is added to the instance's .all_requests attribute. \n",
    "    * For example, below we can see that .n_requests = 1 after both the first and second payload (saved in df1), but .all_requests is 3. \n",
    "* The .all_requests attribute will be used for ensurign compliance with rate limits.\n",
    "    * This could be done directly through *searchtweets*, which has built-in tools (e.g. exponential back-off), by making a single query for the whole period (~280 days).\n",
    "    * However, since the period we are interested in covering here is quite big, this is probably not a good solutions (e.g. if something fails on request 5,000/7,000 all data is lost but all tweets already accessed will count towards the monthly rate limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1319458440633339904</td>\n",
       "      <td>2020-10-23T01:59:41.000Z</td>\n",
       "      <td>Many of the jobs lost this year will never com...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319458438943019009</td>\n",
       "      <td>2020-10-23T01:59:40.000Z</td>\n",
       "      <td>I have a 401K and investments. I like seeing t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1319458252460085249</td>\n",
       "      <td>2020-10-23T01:58:56.000Z</td>\n",
       "      <td>You’re all sitting here while you’re still all...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1319458198932381697</td>\n",
       "      <td>2020-10-23T01:58:43.000Z</td>\n",
       "      <td>In case you didn’t know this, the stock market...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1319458079759568896</td>\n",
       "      <td>2020-10-23T01:58:15.000Z</td>\n",
       "      <td>@Jaycaleb8 Bro what are you saying, I live dow...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1319444682720661504</td>\n",
       "      <td>2020-10-23T01:05:01.000Z</td>\n",
       "      <td>Looking forward to hearing @JoeBiden talk abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1319444365014609920</td>\n",
       "      <td>2020-10-23T01:03:45.000Z</td>\n",
       "      <td>If Biden thinks Trump mishandled Covid 19 he n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1319444125972914176</td>\n",
       "      <td>2020-10-23T01:02:48.000Z</td>\n",
       "      <td>@rrt003 @MSNBC @DailyNewsSA Well no. Also how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1319443992451248129</td>\n",
       "      <td>2020-10-23T01:02:16.000Z</td>\n",
       "      <td>Gays dont not liking Trump as a person. Ooohh ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1319443807641825280</td>\n",
       "      <td>2020-10-23T01:01:32.000Z</td>\n",
       "      <td>@LindseyGrahamSC All economists say @JoeBiden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                created_at  \\\n",
       "0   1319458440633339904  2020-10-23T01:59:41.000Z   \n",
       "1   1319458438943019009  2020-10-23T01:59:40.000Z   \n",
       "2   1319458252460085249  2020-10-23T01:58:56.000Z   \n",
       "3   1319458198932381697  2020-10-23T01:58:43.000Z   \n",
       "4   1319458079759568896  2020-10-23T01:58:15.000Z   \n",
       "..                  ...                       ...   \n",
       "59  1319444682720661504  2020-10-23T01:05:01.000Z   \n",
       "60  1319444365014609920  2020-10-23T01:03:45.000Z   \n",
       "61  1319444125972914176  2020-10-23T01:02:48.000Z   \n",
       "62  1319443992451248129  2020-10-23T01:02:16.000Z   \n",
       "63  1319443807641825280  2020-10-23T01:01:32.000Z   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Many of the jobs lost this year will never com...   \n",
       "1   I have a 401K and investments. I like seeing t...   \n",
       "2   You’re all sitting here while you’re still all...   \n",
       "3   In case you didn’t know this, the stock market...   \n",
       "4   @Jaycaleb8 Bro what are you saying, I live dow...   \n",
       "..                                                ...   \n",
       "59  Looking forward to hearing @JoeBiden talk abou...   \n",
       "60  If Biden thinks Trump mishandled Covid 19 he n...   \n",
       "61  @rrt003 @MSNBC @DailyNewsSA Well no. Also how ...   \n",
       "62  Gays dont not liking Trump as a person. Ooohh ...   \n",
       "63  @LindseyGrahamSC All economists say @JoeBiden ...   \n",
       "\n",
       "    public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                              0                           1   \n",
       "1                              0                           1   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           1   \n",
       "..                           ...                         ...   \n",
       "59                             0                           0   \n",
       "60                             0                           0   \n",
       "61                             0                           1   \n",
       "62                             2                           4   \n",
       "63                             0                           0   \n",
       "\n",
       "    public_metrics.like_count  public_metrics.quote_count  \n",
       "0                           2                           0  \n",
       "1                           1                           0  \n",
       "2                           0                           0  \n",
       "3                           1                           0  \n",
       "4                           0                           0  \n",
       "..                        ...                         ...  \n",
       "59                          2                           0  \n",
       "60                          0                           0  \n",
       "61                          1                           0  \n",
       "62                         40                           1  \n",
       "63                          0                           0  \n",
       "\n",
       "[64 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1.build_query('jobs','2020-10-23 01:00', '2020-10-23 2:00', results_per_call=500)\n",
    "search1.get_data(nTweets = 1000)\n",
    "df2 = search1.get_df()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(search1.rs.n_requests)\n",
    "print(search1.rs.session_request_counter)\n",
    "print(search1.all_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above also gives an indication of how many tweets to expect for our most basic query on 'jobs'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single query: getting data for 1 week based on the data collection periods in the surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey periods\n",
    "# These hold the dates on which data was collected for each survey in part\n",
    "# Will use it to get twitter data from the same period.\n",
    "surveyPeriods = pd.read_excel('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Surveys/table_details/surveyPeriods.xlsx', sheet_name='AI+HPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week start(Monday): 2020-10-19 \n",
      "Week end (Sunday): 2020-10-25\n",
      "Var type: <class 'str'>,<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Use:\n",
    "p1_start, p2_start = twitterData.weekFromDay(surveyPeriods['A_I_start_date'][0])\n",
    "print(\"Week start(Monday): {} \\nWeek end (Sunday): {}\\nVar type: {},{}\".format(p1_start, p2_start, type(p1_start), type(p2_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = twitterData('/Volumes/Survey_Social_Media_Compare/Methods/Scripts/Twitter/')\n",
    "search2.validate_credentials()\n",
    "search2.build_query('jobs', p1_start, p2_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "search2.get_data(nTweets=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1353128768211013634</td>\n",
       "      <td>@VerySaucySalsa @conniechansf Every city that ...</td>\n",
       "      <td>2021-01-23T23:53:32.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1353128719250989057</td>\n",
       "      <td>@GetUpESPN @BartScott57 @Realrclark25 Come on ...</td>\n",
       "      <td>2021-01-23T23:53:21.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1353128517781905408</td>\n",
       "      <td>You’ll raise their pay and the minimum wage fo...</td>\n",
       "      <td>2021-01-23T23:52:33.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1353128220569329665</td>\n",
       "      <td>@redsteeze @mt_lass Thanks New Mexico! We’re l...</td>\n",
       "      <td>2021-01-23T23:51:22.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1353127984576794627</td>\n",
       "      <td>Biden has managed to rid this nation of 100's ...</td>\n",
       "      <td>2021-01-23T23:50:26.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>1350958697183404043</td>\n",
       "      <td>@jussbryant Exactly, so with that being said m...</td>\n",
       "      <td>2021-01-18T00:10:27.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>1350957711375130635</td>\n",
       "      <td>@alavelle07 @laurenboebert So you like Sociali...</td>\n",
       "      <td>2021-01-18T00:06:32.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>1350957355752681474</td>\n",
       "      <td>Or is that what you think, my brotha? How many...</td>\n",
       "      <td>2021-01-18T00:05:07.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>1350956550605725697</td>\n",
       "      <td>@ViolationsGreg @davecokin But the last two OC...</td>\n",
       "      <td>2021-01-18T00:01:55.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>1350956310787981313</td>\n",
       "      <td>@KennyBlankens16 The judges didn’t do their da...</td>\n",
       "      <td>2021-01-18T00:00:58.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5471 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0     1353128768211013634  @VerySaucySalsa @conniechansf Every city that ...   \n",
       "1     1353128719250989057  @GetUpESPN @BartScott57 @Realrclark25 Come on ...   \n",
       "2     1353128517781905408  You’ll raise their pay and the minimum wage fo...   \n",
       "3     1353128220569329665  @redsteeze @mt_lass Thanks New Mexico! We’re l...   \n",
       "4     1353127984576794627  Biden has managed to rid this nation of 100's ...   \n",
       "...                   ...                                                ...   \n",
       "5466  1350958697183404043  @jussbryant Exactly, so with that being said m...   \n",
       "5467  1350957711375130635  @alavelle07 @laurenboebert So you like Sociali...   \n",
       "5468  1350957355752681474  Or is that what you think, my brotha? How many...   \n",
       "5469  1350956550605725697  @ViolationsGreg @davecokin But the last two OC...   \n",
       "5470  1350956310787981313  @KennyBlankens16 The judges didn’t do their da...   \n",
       "\n",
       "                    created_at  public_metrics.retweet_count  \\\n",
       "0     2021-01-23T23:53:32.000Z                             0   \n",
       "1     2021-01-23T23:53:21.000Z                             0   \n",
       "2     2021-01-23T23:52:33.000Z                             0   \n",
       "3     2021-01-23T23:51:22.000Z                             1   \n",
       "4     2021-01-23T23:50:26.000Z                             0   \n",
       "...                        ...                           ...   \n",
       "5466  2021-01-18T00:10:27.000Z                             0   \n",
       "5467  2021-01-18T00:06:32.000Z                             0   \n",
       "5468  2021-01-18T00:05:07.000Z                             0   \n",
       "5469  2021-01-18T00:01:55.000Z                             0   \n",
       "5470  2021-01-18T00:00:58.000Z                             0   \n",
       "\n",
       "      public_metrics.reply_count  public_metrics.like_count  \\\n",
       "0                              0                          1   \n",
       "1                              0                          0   \n",
       "2                              1                          1   \n",
       "3                              0                          0   \n",
       "4                              0                          0   \n",
       "...                          ...                        ...   \n",
       "5466                           1                          0   \n",
       "5467                           0                          0   \n",
       "5468                           0                          0   \n",
       "5469                           1                          0   \n",
       "5470                           1                          0   \n",
       "\n",
       "      public_metrics.quote_count  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "...                          ...  \n",
       "5466                           0  \n",
       "5467                           0  \n",
       "5468                           0  \n",
       "5469                           0  \n",
       "5470                           0  \n",
       "\n",
       "[5471 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search2.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitterSearch",
   "language": "python",
   "name": "twittersearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
